---
layout: post   
title: (Towards Large Reasoning Models) A Survey of Reinforced Reasoning with Large Language Models    
subtitle: AI Survey     
tags: [ai, ml, Reasoning Models, Reinforced Reasoning, LLM]
comments: false  
---

1.	언어와 인간 추론의 관계:
인간의 추론 과정에서 언어가 중요한 역할을 해왔으며, LLM의 발전으로 복잡한 추론 문제에 도전할 수 있는 가능성이 열렸습니다.
2.	‘생각(Thought)’ 개념 도입:
기존의 단순한 자동회귀 토큰 생성 방식을 넘어, 중간 추론 단계를 나타내는 토큰 시퀀스인 ‘생각’을 도입함으로써 트리 탐색, 반성적 사고 등 복잡한 인간의 추론 과정을 모방할 수 있게 되었습니다.
3.	강화 학습을 통한 학습-추론:
최근에는 강화 학습(RL)을 활용하여 LLM이 추론 과정을 스스로 학습하도록 하는 접근법이 등장했습니다. 이 방법은 시행착오 기반의 탐색 알고리즘을 통해 고품질의 추론 경로를 자동으로 생성, LLM의 추론 능력을 크게 확장시킵니다.
4.	테스트 시간 확장 기법:
추론 시에 더 많은 토큰을 사용하여 ‘생각’하도록 유도하면, 추론의 정확도를 더욱 높일 수 있음이 여러 연구에서 입증되었습니다. 이는 학습 단계와 테스트 단계에서의 확장이 결합되어 대규모 추론 모델(Large Reasoning Model)로 나아가는 새로운 연구 방향을 제시합니다.
5.	OpenAI o1 시리즈의 중요성:
OpenAI의 o1 시리즈 도입은 이러한 연구 방향에서 중요한 이정표로 평가되며, 대규모 추론 모델 구축에 큰 기여를 하고 있습니다.
6.	전반적 리뷰 및 향후 연구 방향:
본 논문은 LLM의 기본 배경과 함께, 자동 데이터 구성, 학습-추론 기법, 테스트 시간 확장 등 대규모 추론 모델 개발의 핵심 기술 요소들을 자세히 살펴보고, 관련 오픈 소스 프로젝트들을 분석합니다. 마지막으로, 남아있는 도전 과제와 미래 연구 방향에 대해 논의합니다.

[Paper Link](https://arxiv.org/pdf/2501.09686)

## Backgroud

1. 사전학습 (Pre-training)
- 기본 개념:
    LLM의 사전학습은 모델이 핵심 언어 지식과 다양한 세계 지식을 습득하는 기초 단계로, 추론 능력의 토대를 마련합니다.
- 데이터 및 아키텍처:
    - 고품질의 텍스트 코퍼스(웹 콘텐츠, 도서, 코드 등)를 활용하며, Transformer 기반의 다음 토큰 예측(task)을 중심으로 학습합니다.
    - 사전학습 후, LLM은 우수한 in-context 학습 능력을 갖추어 광범위한 질문에 대해 일관된 텍스트 생성과 정답 도출이 가능합니다.
- 추론 능력 강화:
    - 코드 및 수학적 내용이 풍부한 데이터셋이 강건한 추론 능력 개발에 핵심적인 역할을 하며, 최근에는 이러한 전문 데이터를 포함한 합성 데이터도 도입되고 있습니다.
    - 일반 텍스트와 전문 데이터(코드, 수학 등) 간의 균형을 유지하는 것이 중요한 도전 과제로 제시됩니다.

2. 파인튜닝 (Fine-tuning)
- 기본 개념:
    사전학습 단계 후, 지도학습 기반의 파인튜닝(Instruction Tuning)을 통해 모델의 출력 스타일을 인간의 요구와 실제 응용 상황에 맞게 조정합니다.
- 데이터셋 및 기법:
    - 다양한 수작업 주석과 정제 과정을 거친 인스트럭션 데이터셋을 활용하며, ChatGPT의 등장 이후 강력한 LLM으로부터 직접 데이터 증류 및 자동화된 대규모 데이터셋 생성 방법이 도입되었습니다.
    - 파인튜닝은 기본적으로 다음 토큰 예측 목표를 유지하지만, 질문보다는 답변에 대한 손실을 중점적으로 계산합니다.
- 추론 능력 향상:
    - Chain-of-Thought (CoT) 및 수학 문제 해결 사례와 같은 데이터를 포함시킴으로써 모델의 추론 능력을 더욱 강화하는 연구가 진행되고 있습니다.
    - 최신 접근법은 고급 추론 모델로부터 데이터 증류 후 파인튜닝을 통해 최종적인 대규모 추론 모델을 완성하는 전략을 취합니다.

3. 정렬 (Alignment)
- 기본 개념:
    단순한 데이터 증류를 넘어서, 강화학습 기법을 활용한 정렬 단계는 모델이 “유용, 무해, 정직”한 콘텐츠를 생성하도록 유도합니다.
- 주요 기법:
    - RLHF (Reinforcement Learning from Human Feedback): 사람이 주석한 대규모 순위 데이터셋을 활용하여, SFT 모델, 보상 모델, 그리고 정렬 모델을 PPO 등의 기법으로 반복 최적화합니다.
    - DPO (Direct Preference Optimization): 명시적인 보상 모델 의존도를 낮추기 위해, 정책 기반의 선호 손실을 직접 정의하여 모델 최적화를 유도합니다.
- 추론 강화 측면:
    복잡한 추론 문제를 단계적으로 분해하고 피드백 신호를 점진적으로 제공하는 self-training 방식(강화학습 및 선호 학습 기반)이 최종 정렬 단계에서 중요한 역할을 수행합니다.

4. 고급 추론을 위한 프롬프트 (Prompting) 기법
- 핵심 아이디어:
    LLM이 내재하고 있는 추론 능력을 최대한 발휘하기 위해, 입력 프롬프트를 체계적으로 구성하는 다양한 전략이 제안됩니다.
- 주요 방법론:
    - 단계적 추론 (Chain-of-Thought): 중간 추론 단계를 명시적으로 기술함으로써 문제 해결 과정을 투명하게 만들고, “Let’s think step by step”와 같은 단순 프롬프트도 효과적입니다.
    - Self-Consistency: 여러 추론 경로를 생성하여 보다 신뢰성 있는 결론 도출.
    - Auto-CoT: 효과적인 추론 체인을 자동 생성.
    - 다경로 탐색 (Multi-path Exploration):
        - Tree-of-Thought: 추론 경로를 트리 구조로 구성하여 다양한 해결책을 체계적으로 탐색.
        - Graph-of-Thought: 그래프 구조를 통해 보다 유연한 추론 및 역추적(backtracking) 가능.
    - ReAct: 추론 과정에 행동(action) 단계를 결합하여 외부 환경과의 동적 상호작용 지원.
    - 문제 분해 (Decomposition-based Methods):
        - Least-to-Most Prompting, Algorithm of Thoughts, Plan-and-Solve: 복잡한 문제를 관리 가능한 하위 문제로 체계적으로 분해하고 전략적 해결 방안을 제시하여 다단계 분석을 가능하게 함.

5. 에이전틱 워크플로우 (Agentic Workflow)
- 기본 개념:
    LLM의 인스트럭션 따르기와 in-context 학습 능력을 넘어서, “사고 패턴”을 직접 프로그래밍하는 에이전틱 워크플로우가 설계되고 있습니다.
- 특징 및 효과:
    - 추가적인 학습 없이도 테스트 시간에 더 많은 계산 자원을 투입하여 추론 능력을 향상시킬 수 있습니다.
    - in-context 학습은 몇 개의 예시만 제공해도 모델이 새로운 문제에 효과적으로 일반화할 수 있도록 도와주며, 입력 텍스트의 분포, 라벨 공간, 응답 형식 등을 캡처합니다.
    - 이를 통해 LLM은 특정 역할 시뮬레이션, 인간 행동 모방, 인간-LLM 상호작용 및 협력적 문제 해결과 같은 복잡한 인지 작업에서 뛰어난 성능을 보입니다.
- 의의:
    에이전틱 워크플로우는 복잡한 인지 아키텍처와 결합하여 LLM의 고급 추론 능력을 극대화하는 데 핵심적인 역할을 합니다.

## Data Construction: from Human Annotation to LLM Automation

1. 대규모 고품질 추론 데이터셋의 필요성과 도전 과제
- 핵심 목표:
    대규모 언어 모델(LLM)의 추론 능력을 향상시키기 위해, 복잡한 문제 해결 과정을 충분히 반영하는 고품질 데이터셋을 구축하는 것이 필수적입니다.
- 주요 도전 과제:
    - 비용 문제: 인간 주도의 세밀한 데이터 주석(human annotation)은 높은 품질을 보장하지만, 대규모로 구축하기에는 비용과 시간이 많이 듭니다.
    - 자동화의 한계: LLM을 활용한 자동 주석(LLM automation)은 비용 효율적이나, 특히 중간 단계의 추론(즉, step-by-step reasoning) 검증에서 품질 보장이 어려운 문제가 있습니다.

2. 인간 주석 (Human Annotation)
- 정의 및 중요성:
    인간 주석은 전문가나 주석자가 직접 데이터를 꼼꼼하게 라벨링하는 방식입니다.
- 장점:
    - 정밀도와 적응력: 모호하거나 복잡한 데이터를 다룰 때 뛰어난 적응력과 세밀한 판단력을 발휘합니다.
    - 추론 과정 지도: 수학 문제나 복잡한 추론 과정에서 각 단계마다 올바른 판단을 내리도록 도와, 모델이 인간의 사고 패턴을 학습하게 만듭니다.
- 활용 사례:
    - RLHF(인간 피드백을 통한 강화학습)에서 인간의 선호 데이터는 모델이 윤리적·복잡한 가치 판단을 내리도록 정렬(alignment)하는 데 큰 역할을 합니다.
    - 수학적 추론 작업에서 각 단계별 주석을 제공하여 모델의 추론 정확도를 높이는 연구가 진행되고 있습니다.
- 한계 및 보완 전략:
    - 대규모 데이터 확보의 어려움: LLM 학습에는 수 테라바이트에 달하는 데이터가 필요하므로, 순수 인간 주석만으로는 확장이 어렵습니다.
    - 인간-LLM 협업:
        - 전처리(Pre-annotation) 단계: LLM이 미리 주석을 달고,
        - 정제(Refinement) 단계: 인간 주석자가 LLM이 생성한 주석 중 품질이 낮은 부분만 선별하여 수정하는 방식으로, 자동화와 인간의 세밀함을 결합합니다.

3. LLM을 활용한 자동 결과 주석 (LLM Automated Outcome Annotation)
- 목표 및 접근법:
    LLM 자체의 강력한 문맥 이해와 추론 능력을 활용하여, 데이터의 최종 결과나 주요 정보(예: 질문-답변 추출, 도구 사용 등)를 자동으로 주석화하는 방법입니다.
- 주요 특징:
    - 대용량 텍스트 처리: 긴 문서나 대규모 구조화 데이터를 한 번에 처리할 수 있어, 자동 주석 작업에서 효율성을 높입니다.
    - 명령어 따르기 능력: LLM은 다양한 복잡한 주석 시나리오에 유연하게 대응할 수 있으며, 인간이 제공한 예시(데모)를 모방하여 복잡한 추론 경로나 중간 단계를 자동으로 생성할 수 있습니다.
    - 피드백 기반 개선:
        - 자기 수정(Self-refinement): 모델이 잘못된 주석을 스스로 인식하고, 오류 데이터를 피드백 받아 개선합니다.
        - 대조 학습(Contrastive Learning): 실패한 주석 경로와 성공한 경로를 비교하여, 더 나은 주석 기준을 학습하는 방식입니다.

4. LLM을 활용한 자동 프로세스 주석 (LLM Automated Process Annotation)
- 정의 및 필요성:
    단순히 최종 결과만 주석화하는 것이 아니라, 모델이 문제를 해결하는 중간 과정의 각 단계에 대해 “정확함”, “오류” 또는 중간 보상 등으로 평가(라벨링)하는 작업입니다.
- **중간 주석(process annotation)**은 복잡한 추론 문제에서 각 단계가 최종 결과에 미치는 영향을 체계적으로 분석하기 위해 필요합니다.
- 자동화 방법:
	1.	더 강력한 외부 LLM 사용:
    - 강력한 모델(예: 최신 GPT 계열)을 사용하여, 상대적으로 약한 LLM이 생성한 중간 결과를 평가합니다.
    - 한계: 외부 모델의 성능에 의존하기 때문에, 그 한계가 전체 주석 품질에 영향을 줄 수 있습니다.
	2.	몬테카를로 시뮬레이션 (Monte Carlo Simulation):
    - 중간 단계에서 여러 번 무작위 추론을 실행하여, 그 평균 결과를 바탕으로 해당 단계의 품질을 평가합니다.
    - 장점: 단일 평가 대신 여러 번의 확률적 평가를 통해 신뢰도를 높입니다.
	3.	몬테카를로 트리 탐색 (Monte Carlo Tree Search, MCTS):
    - 여러 결과(leaf node)를 트리 구조로 생성하여, 각 중간 단계의 평가를 보다 체계적으로 수행합니다.
    - 효과: 무작위 시뮬레이션보다 계산 효율성이 높으며, 고품질의 부모 노드 정보를 공유하여 전반적인 평가 정확도를 개선합니다.
	4.	자기 정제(self-refining) 메커니즘:
    - 위에서 얻은 프로세스 주석을 기반으로 **프로세스 보상 모델(Process Reward Model, PRM)**을 학습합니다.
    - 학습된 PRM을 활용하여 LLM의 추론 과정과 주석 품질을 반복적으로 개선하는 순환 구조를 만듭니다.

## Learning to Reason: from Supervised to Reinforcement Fine-tuning

1. 배경 및 필요성
- 사전학습 모델의 한계:
    대규모 사전학습(pre-trained) 모델은 다양한 작업에서 뛰어난 성능을 보이나, 복잡한 추론 문제 해결과 인간 기대에 부합하는 출력 정렬(alignment)에는 한계가 있음.
- Fine-tuning의 역할:
    특정 작업에 맞춰 모델의 성능을 미세 조정(fine-tuning)함으로써, 복잡한 다단계 추론 및 인간의 기대에 부합하는 출력을 생성하도록 개선할 수 있음.

2. 지도학습 기반 Fine-tuning (Supervised Fine-Tuning, SFT)
- 기본 개념:
    - SFT는 대규모 비지도 학습을 통해 획득한 일반적인 언어 이해 능력을, 라벨링된(task-specific) 데이터셋을 통해 특정 도메인이나 작업에 맞게 정제하는 과정임.
    - 대표적인 사전학습 모델(GPT, BERT, T5 등)은 방대한 양의 텍스트 데이터를 기반으로 기본 능력을 갖추지만, 작업별 요구사항에 맞는 정밀한 출력을 위해 SFT가 필수적임.
- 추론 능력 강화:
    - Chain-of-Thought (CoT) 기법: 모델이 최종 답변에 도달하기 전에 중간 추론 단계를 명시적으로 생성하도록 학습시켜, 문제 해결 과정을 투명하게 하고 복잡한 논리적 문제를 효과적으로 처리하도록 도움.
    - 연구 사례에서는 CoT 기반 SFT를 통해 수학 문제, 논리적 추론 등에서 모델의 성능과 해석 가능성이 크게 향상됨.
- 한계점:
    - 데이터 의존성: 고품질의 라벨링 데이터가 필요하며, 이는 비용과 시간이 많이 소요됨.
    - Catastrophic Forgetting: 파인튜닝 과정에서 사전학습 단계의 일반적인 지식이 일부 소실될 수 있음.
    - 계산 비용: 대규모 모델을 미세 조정하는 데 필요한 자원도 여전히 부담으로 작용할 수 있음.

3. 강화학습 기반 Fine-tuning
- 기본 개념:
    강화학습(Reinforcement Learning, RL)은 모델이 시행착오(trial and error)를 통해 보상(reward) 신호를 받아 최적의 행동 전략을 학습하는 접근법으로, 복잡한 추론 과제에 적합함.
- 주요 RL 기법:
    - Reinforcement Learning from Human Feedback (RLHF):
        - 인간이 제공한 선호 데이터(예: 순위 비교)를 기반으로 모델의 출력이 인간의 의도와 윤리적 기준에 부합하도록 학습시킴.
        - GPT-3 등에서 실제로 적용되어, 상대적으로 작은 모델이더라도 뛰어난 추론 및 지시 수행 능력을 보임.
    - Reinforcement Learning from AI Feedback (RLAIF):
        - 인간 피드백 대신, 사전에 정의된 원칙(예: Constitutional AI)이나 모델 자체의 평가를 활용해 학습하는 방법.
        - 이를 통해 고가의 인간 주석 데이터 의존도를 낮추고, 모델이 스스로 출력을 개선하도록 유도함.
    - Direct Preference Optimization (DPO):
        - RL 기반 방법들이 복잡한 보상 모델을 학습하는 데 반해, DPO는 인간 선호 데이터를 직접 활용하여 두 출력 간의 선호 관계를 비교함으로써 모델 정책(policy)을 최적화함.
        - 장점:
            - 복잡한 보상 함수나 샘플링 과정이 필요 없으므로, 안정적이며 계산 비용도 낮음.
    - DPO의 확장 버전인 ODPO(Direct Preference Optimization with an Offset)는 선호 정도의 차이를 고려하여 더욱 정밀하게 모델의 출력 정렬과 추론 능력을 강화함.
- 학습 과정의 특성:
    - RL 및 DPO 방법은 각 행동에 대해 즉각적인 보상을 부여하는 방식으로, 긴 시퀀스 전체에 대한 복잡한 신호 할당 문제(credit assignment)를 단순화함.
    - 이러한 단기 피드백(short-term feedback)에 초점을 맞추어, 실시간 응용 및 명료한 추론 과정이 요구되는 작업에 효과적임.


4. 결과 보상 모델(Outcome Reward Model, ORM)을 통한 다단계 추론 향상

배경 및 문제점
    - 복잡한 추론 작업에서는 모델이 Chain-of-Thought와 같이 여러 중간 단계를 거쳐 최종 답변에 도달하는데, 이때 보상 신호는 최종 결과가 도출된 후에만 제공됩니다.
    - 핵심 과제는 최종 결과에 따른 보상 신호를 기반으로 각 중간 추론 단계의 정확성과 기여도를 평가하여, 올바른 추론 경로를 강화하는 것입니다.

주요 접근법 및 사례
- PPO 기반 방법 (ReFT)
    - 일반적인 PPO에서는 최종 보상 신호만으로 학습하는 경향이 있어, 긴 추론 경로 내에서 각 중간 단계의 기여도를 정확히 평가하기 어려운 신용 할당(credit assignment) 문제가 발생
    - ReFT는 모델이 최종 보상에 도달하기 전의 다양한 추론 경로를 탐색하고, 각 단계의 상대적 기여도를 추정하여 보다 다양한 reasoning path를 학습할 수 있도록 함
    - VinePPO는 중간 단계 평가에서 발생하는 편향 문제를 줄이기 위해 값(value) 네트워크 대신 몬테카를로(Monte Carlo) 샘플링 방식을 도입
        - 여러 번의 무작위 샘플링을 통해 각 단계의 기여도를 평균화하여 보다 편향 없는 가치 추정을 가능하게 함
- Critical Plan Step Learning (CPL)
    - 다단계 추론 문제에서 모델이 단순히 최종 답변만 생성하는 것이 아니라, 문제 해결 과정 중 고수준의 계획 단계(plan step)를 명시적으로 탐색하도록 하는 방법론
    - CPL은 다단계 추론에서 고수준의 계획 단계(plan step)를 탐색하기 위해 몬테카를로 트리 탐색(MCTS)을 활용합니다.
    - Step-APO 기법을 통해 핵심 계획 단계를 학습하고, 정책(어떤 행동을 취할지 결정) 및 값(각 단계의 중요도 평가) 모델을 반복적으로 업데이트하여 다양한 추론 경로를 학습, 일반화 능력을 향상시킵니다.
- Direct Preference Optimization (DPO) 개선
    - 기존 DPO 방식은 긴 추론 경로로 인해 성능이 저하되는 한계가 있으므로, ODPO(Direct Preference Optimization with an Offset)가 도입되어, 응답 간 선호 정도의 차이를 반영(Offset)해 보다 정밀한 모델 정렬을 구현


5. 과정 보상 모델(Process Reward Model, PRM)을 통한 다단계 추론 향상 

배경 및 필요성
- **과정 보상 모델(PRM)**은 최종 결과에만 의존하는 대신, 추론의 각 중간 단계마다 세분화된 보상을 제공하여, 모델이 전체 추론 경로를 보다 미세하게 조정하도록 돕습니다.
- 이러한 세밀한 피드백은 순차적 의사결정 및 복잡한 문제 해결에서 중간 단계의 논리적 진행과 정확성을 강화하는 데 필수적입니다.

주요 접근법 및 사례
- 수학 및 물류 추론에서의 적용
    - SELF-EXPLORE: 초기 오류(“first pits”)를 보정하는 단계를 보상하여, 자기 지도 학습(self-supervised fine-tuning)을 통해 수학 문제 해결 정확도를 크게 향상시킵니다.
        - 모델은 주어진 문제에 대해 여러 개의 추론 경로를 샘플링하고, 각 경로에서 최종 결과가 정답에 도달하지 못하는 경우 처음으로 잘못된 단계(“first pit”)를 기록
        - 이 “first pit”에 대해 부정적인 보상을 부여하고, 올바른 단계에 대해서는 긍정적인 보상을 부여하여, 모델이 향후 유사 상황에서 오류를 피하도록 학습
        - 모델 자체의 출력을 활용하여 보상 신호를 생성함으로써, 학습 데이터의 확장성과 비용 효율성을 크게 향상
    - MATH-SHEPHERD: MCTS 기반의 자동화된 과정 감독을 도입하여, 인간 주석 없이도 단계별 검증 및 보상이 이루어지도록 설계되어 높은 정확도를 달성합니다.
        - 각 중간 단계에서 여러 추론 경로를 생성합니다. 그 결과, 각 단계의 “잠재적 올바름”을 정량화하여 보상 신호로 활용
        - 각 단계에 대한 점수를 기반으로 올바른 추론 경로를 재랭킹(reranking)하거나, PPO와 같은 강화학습 기법에 활용
    - DeepSeekMath: GRPO(Group Relative Policy Optimization)를 통해 단계별 보상을 최적화, 다양한 도메인에서 일관된 추론 성능을 기록합니다.
    - Process Advantage Verifiers (PAVs): 단계별 진행 상황을 평가하여, 표본 및 계산 효율성과 추론 정확도를 동시에 개선합니다.
        - 각 단계에서 해당 단계가 정답 도출에 얼마나 기여하는지를 평가합니다. 이를 위해 MCTS 기반의 시뮬레이션을 활용하여, 각 단계의 “진행 이득”을 추정
- 상호작용 및 다턴 대화에의 적용
    - ArCHer: 계층적 강화학습을 통해, 발화(utterance) 단위의 고수준 보상과 토큰 단위의 저수준 보상을 동시에 적용하여 다턴 대화에서 효과적인 신용 할당을 구현합니다.
    - Multi-turn RL from Preference Human Feedback (MTPO): 다턴 상호작용 전체를 비교하여 선호 신호를 생성, 각 단계별로 보상을 할당함으로써 장기 목표와 일치하는 행동을 강화합니다.
    - Direct Preference Optimization과의 결합
        - MCTS를 활용하여 단계별 선호 데이터를 자동으로 수집하고, 이를 통해 명시적 값 모델과 결합한 DPO 기법을 통해 보다 효율적인 추론 경로 탐색 및 정책 최적화를 진행합니다.

6. Reinforcement Fine-tuning (RFT)

OpenAI에서 제안한 기술로, 특정 분야에 특화된 전문가 수준의 LLM을 맞춤화하기 위해 고안되었습니다. 현재 RFT는 연구 프로그램의 일부로 진행 중이며, 기술적 세부 사항은 전부 공개되지 않은 상태입니다. 주요 내용은 다음과 같습니다.
1.	기본 개념:
- RFT는 소량의 사용자 선호 데이터를 활용하여 LLM의 출력에 대해 평가를 수행하는 그레이더(grader) 모델을 도입합니다.
- 이를 통해 다단계 추론 과정에서 발생하는 문제들을 반복적으로 최적화하며, 유사 문제 해결 전략을 강화할 수 있습니다.
2.	그레이더 모델:
- 그레이더는 전통적인 보상 모델(reward model)과 유사한 역할을 수행하며, 질문과 답변 등 텍스트 입력을 받아 추론 품질을 나타내는 스칼라 값으로 변환합니다.
- 이 모델은 사용자 제공 선호 데이터를 기반으로 학습되며, 결과 보상 모델(Outcome Reward Model) 또는 과정 보상 모델(Process Reward Model)로 작동할 가능성이 있습니다.
3.	데이터 효율성:
- OpenAI의 공개 세션에서, RFT는 수십 개의 사용자 선호 데이터만으로도 새로운 도메인에서 효과적인 학습이 가능함이 언급되었습니다.
- 이는 제한된 데이터만으로도 다양한 추론 경로를 탐색하고, 과적합의 위험을 줄이는 매우 높은 샘플 효율성을 시사합니다.
4.	학습 안정성:
- 강화학습(RL) 기반 학습은 무작위 시드나 하이퍼파라미터 조정에 따라 결과가 크게 달라지는 불안정성이 존재하지만, RFT는 이러한 문제를 어느 정도 해결한 것으로 보입니다.
- OpenAI는 해당 기술을 API 형태로 공개할 계획임을 밝힘으로써, RFT가 RL 기법을 이용한 안정적인 LLM 미세조정에 필요한 수준의 안정성을 달성했음을 시사합니다.

## Test-time Scaling: from CoTs to PRM Guided Search

1. 테스트 타임 프롬프트 기법: 의도적 사고 유도
- 핵심 개념:
    학습 단계(Train-time)에서의 최적화 외에도, 테스트 타임(Test-time) 프롬프트 기법—예를 들어, Chain-of-Thought (CoT) 및 Tree-of-Thoughts와 같은 구조화된 프롬프트—를 통해 LLM의 추론 능력을 추가로 향상시킬 수 있습니다.
- 세부 내용:
    - 명시적 추론 과정 유도: 단순히 모델에 정답을 직접 요청하는 대신, 모델이 중간 추론 단계를 생성하도록 유도함으로써 보다 체계적이고 해석 가능한 사고 과정이 촉진됩니다.
    - 구조화된 프롬프트 방법론: ReAct, Least-to-Most Prompting 등과 같은 기법은 모델이 스스로 생각의 흐름을 정리하도록 도와주어, 문제 해결 과정의 신뢰성과 정확도를 높입니다.
    - 비용-효과: 이러한 방법은 토큰 소비와 계산 오버헤드를 증가시키지만, 모델 파라미터의 수정 없이 추론 능력을 강화할 수 있는 유력한 보완 수단을 제공합니다.

2. PRM Guided Search: 세밀한 과정 기반 탐색
- 핵심 개념:
    Process Reward Model (PRM)은 기존의 최종 결과에 의존하는 피드백 방식에서 벗어나, 추론의 각 중간 단계에 대해 세밀한 보상을 제공하는 접근법입니다. PRM은 테스트 타임에도 활용되어 모델의 추론 능력을 더욱 향상시킬 수 있습니다.
- 세부 내용:
    - PRM의 역할: PRM은 희소한 결과 기반의 피드백 대신, 각 추론 단계별로 보상을 산출하여 모델이 보다 체계적으로 오류를 수정하고 올바른 경로를 선택할 수 있도록 지원합니다.
    - 테스트 타임 탐색 전략:
        - Majority Vote: 다수결 방식으로 여러 추론 경로의 결과를 집계하여 최종 출력을 결정합니다.
        - Tree Search (MCTS): 체계적으로 추론 경로를 확장하는 트리 구조를 구축하여, 다양한 경로 중 최적의 해법을 탐색합니다.
        - Beam Search: 각 단계에서 상위 K개의 후보를 유지하며 탐색 폭을 확장하여, 단순 탐욕적 검색(greedy search)보다 우수한 출력 품질을 확보합니다.
        - Lookahead Search: Beam Search의 확장 기법으로, 각 단계에서 k 스텝을 미리 시뮬레이션하여 누적 보상에 기반해 후보를 평가합니다. 이를 통해 보다 멀리 내다보며 현재의 선택을 보정할 수 있으나, 계산 비용이 증가합니다.
- 미래 전망: 테스트 타임 계산 자원을 확장하는 새로운 scaling 법칙에 따라, PRM 기반 탐색은 LLM의 추론 능력을 지속적으로 향상시킬 수 있는 유망한 연구 방향을 제시합니다.

* MCTS의 4가지 주요 단계
1.	선택 (Selection)
- 과정:
트리의 루트 노드(현재 상태)에서 시작하여, 현재까지의 경험(방문 횟수, 평균 보상 등)을 바탕으로 자식 노드 중 가장 유망한 노드를 선택합니다.
- 기법:
일반적으로 UCT(Upper Confidence Bound for Trees) 공식을 사용하여 각 노드의 탐색/활용 균형을 평가합니다.
- 예시:
체스 게임에서 현재 체스판 상태를 루트 노드로 두고, 가능한 다음 수들(예: 나이트 이동, 폰 전진 등)에 대해 UCT 점수를 계산하여, 가장 높은 점수를 가진 움직임을 선택합니다.
2.	확장 (Expansion)
- 과정:
선택 단계에서 도달한 리프 노드(아직 충분한 탐색이 이루어지지 않은 노드)에서 새로운 자식 노드를 추가합니다.
- 예시:
체스에서 리프 노드에 도달한 후, 아직 탐색되지 않은 새로운 움직임(예: 비숍 이동)을 선택하여 해당 상태를 자식 노드로 추가합니다.
3.	시뮬레이션 (Simulation)
- 과정:
새로 확장된 노드부터 시작하여, 끝까지 무작위로 행동을 선택하며 시뮬레이션을 진행합니다. 이 과정을 통해 최종 상태(예: 승리, 무승부, 패배)를 도출하고, 그 결과에 따라 보상을 얻습니다.
- 예시:
새로 추가된 “비숍 이동” 노드에서부터 무작위로 행동을 선택해 체스 게임을 끝까지 진행하고, 그 결과로 승리(보상 +1) 또는 패배(보상 0 또는 -1)와 같은 보상을 산출합니다.
4.	역전파 (Backpropagation)
- 과정:
시뮬레이션에서 얻은 보상 값을 확장한 노드에서부터 루트 노드까지 거슬러 올라가며 업데이트합니다. 각 노드의 방문 횟수와 평균 보상 값을 갱신하여, 향후 선택 단계에서 더 정확한 판단이 가능하도록 합니다.
- 예시:
“비숍 이동”을 확장한 노드에서의 시뮬레이션 결과 승리 보상을 받은 경우, 해당 노드 및 그 조상 노드들의 방문 횟수를 증가시키고, 평균 보상을 업데이트합니다.

## Path toward Large Reasoning Model

1. OpenAI o1 시리즈의 발전

OpenAI o1 시리즈는 2024년 9월에 출시된 혁신적인 언어 모델로, 복잡한 수학, 코딩, 과학 문제 해결 등 다양한 분야에서 탁월한 추론 능력을 보여줍니다. 이후 2024년 12월에는 o1의 업그레이드 버전인 o3가 공개되었으며, 이 모델은 박사 수준의 지능을 보유한 것으로 평가됩니다.

주요 성과 및 특징은 다음과 같습니다:
- 효과적인 지식 통합:
o1은 단계별 논리적 추론을 통한 문제 분해와 공식적 유도 과정을 체계적으로 수행합니다. 경쟁 프로그래밍 분야에서 83.3%의 성공률을 달성했으며, 방사선학이나 칩 설계와 같이 여러 도메인 지식을 융합해야 하는 문제에서도 뛰어난 성능을 입증합니다. 연구 결과에 따르면, 구조적 분석 및 계산 추론 작업에서 인간 수준의 150%에 해당하는 성능을 보여줍니다.
- 체계적인 문제 분해:
모델은 다양한 난이도의 문제에서도 일관되게 핵심 원리와 증명을 식별하며, 단계별로 문제를 분해하는 능력을 유지합니다. 수학적 추론에서는 네덜란드 수학 시험 등에서 거의 만점에 가까운 성적을 기록했으며, 프로그래밍 디버깅에서도 오류 식별, 원인 분석, 수정의 세 단계 접근법을 통해 안정적인 성능을 보입니다.
- 신뢰할 수 있는 복잡한 문제 해결:
o1은 다양한 문제 유형(예: 계획 수립, 불완전 정보, 동적 제약 조건 등)에서 추론 체인의 일관성과 신뢰성을 유지하며, 장기적 계획 문제에서도 우수한 성능을 나타냅니다. 이는 시간적 추론 및 인과 관계 이해 능력이 탁월함을 의미합니다.
- 새로운 스케일링 법칙:
대규모 강화학습을 통한 훈련 과정과 테스트 타임 계산 자원의 효율적 배분을 통해, 추가적인 ‘생각하는 시간’을 부여할 경우 성능이 지속적으로 향상됨을 보였습니다. 예를 들어, 프로그래밍 문제에서는 10,000회 제출 허용 시 금메달 기준 이상의 결과를 도출할 수 있었으며, 이는 모델 크기를 획기적으로 늘리지 않고도 높은 추론 성능을 달성할 수 있음을 시사합니다.

2. 오픈소스 기반 대규모 추론 모델 개발 시도

오픈소스 커뮤니티에서도 OpenAI o1과 유사한 수준의 추론 능력을 목표로 다양한 접근법이 시도되고 있습니다. 주요 오픈소스 프로젝트들은 각기 다른 방법론을 활용하여 데이터 구축, 사전 학습, 사후 미세조정 및 테스트 타임 개선을 이루고 있습니다. 예를 들어:
- OpenR:
    - 데이터 구축: MCTS를 활용하여 단계별 추론 데이터를 생성합니다.
    - 사후 미세조정: GRPO를 적용하여 정책을 업데이트하며,
    - 테스트 타임: Best-of-N 전략을 통해 최종 출력을 선택합니다.
- Rest-MCTS:
    - 데이터 구축: 내부 루프에서 MCTS를 적용하여 데이터를 확장하고,
    - 사후 미세조정: Supervised Fine-Tuning(SFT)을 수행하며,
    - 테스트 타임: Tree Search 기법을 이용해 최적의 결과를 탐색합니다.
- Journey Learning:
    - 미세조정: SFT를 통해 모델을 학습시키며,
    - 테스트 타임: Tree Search 기법을 적용하여 추론 경로를 탐색합니다.
- LLaMA-Berry:
    - 데이터 구축: MCTS 기반으로 데이터를 수집한 후,
    - 사후 미세조정: Direct Preference Optimization (DPO)를 통해 모델을 정렬하고,
    - 테스트 타임: Enhanced Borda Count (EBC) 방식을 사용해 후보 답변의 순위를 결정합니다.

이러한 다양한 접근법은 각기 다른 단계(데이터 구축, 사전 학습, 사후 미세조정, 테스트 타임 개선)를 독자적으로 혹은 결합하여, 대규모 LLM의 추론 능력을 강화하는 데 기여하고 있습니다.

## Other Test-time Enhancing Techniques

1. 개요

테스트 타임에서 LLM의 추론 능력을 향상시키기 위한 다양한 기법들이 제안되고 있습니다. 이 기법들은 모델 파라미터를 수정하지 않고도, 추가적인 계산 자원을 활용하여 출력 결과를 동적으로 개선합니다. 여기에는 PRM 기반 탐색 외에도 Verbal Reinforcement Search, Memory-based Reinforcement, Agentic System Search 등 여러 방법이 포함됩니다.

2. Verbal Reinforcement Search (VRS)
- 기본 원리:
    VRS는 LLM이 이미 내재한 추론 및 의미론적 능력을 활용하여, 테스트 타임에 반복적 피드백 루프를 통해 솔루션 공간을 탐색 및 최적화합니다.
- 특징 및 응용:
    - 개별 에이전트 환경:
        - 반복적 추론 및 피드백을 통해 수학적 최적화, 기호 추론, 가설 기반 탐구 등에서 문제 해결 능력을 향상시킵니다.
        - 예를 들어, 조합 문제(캡셋, 온라인 빈-패킹 문제)나 기호 회귀(symbolic regression) 문제에서, VRS를 통한 반복 평가가 기존 최적화 기법보다 효율적이고 정확한 결과를 도출합니다.
- 다중 에이전트 시스템:
    - LLM 기반 에이전트 간 자연어 소통을 통해 공동 추론 및 피드백을 주고받아 복잡한 문제 해결에 기여합니다.
    - 메타 구조 발견, 인과 관계 추론, 사회경제적 예측 등에서 에이전트들이 협업하여 성능을 향상시킵니다.
- 실제 환경(Embodied Agent) 적용:
    - 물리적 상호작용이 필요한 실험 계획 및 실행 등에서, LLM 에이전트가 실제 장비(예: 로봇, 실험 장치)와 연계해 반복적 피드백을 통해 최적의 실행 전략을 수립합니다.
    - 예를 들어, 자율화학 연구에서 촉매 반응 최적화 문제를 해결하는 과정에서, 잘못된 모듈 호출 등 오류 발생 시 문서화와 재시도를 통해 문제를 수정합니다.

3. Memory-based Reinforcement
- 기본 원리:
    복잡하고 개방된 문제(예: 창의적 글쓰기, 복잡한 논리 추론, 오픈월드 게임)에서는 솔루션 공간이 매우 확장되어 단순 탐색으로는 한계가 있습니다. 이에, 외부 메모리 모듈을 도입하여 에이전트가 이전의 관찰, 성공/실패 사례 등을 저장하고 이를 기반으로 반복적 학습을 수행하도록 합니다.
- 세부 분류:
	1.	Experiential Learning:
    - 에이전트가 메모리에 저장된 긍정적 경험을 모방하고, 부정적 경험은 회피하도록 학습합니다.
    - 예시: REMEMBERER, Memory Sharing, Experiential Co-Learning
	2.	Reflective Learning:
    - 단순한 사례 제공을 넘어서, 에이전트가 과거 성공/실패의 원인을 직접 반영하여 이를 요약 및 내재화하도록 합니다.
    - 예시: Reflexion, ExpeL, RAHL
	3.	Concept Learning:
    - 에이전트가 특정 작업에 국한되지 않고, 일반화된 “개념”을 학습해 환경과 과제에 대한 폭넓은 이해를 구축하도록 합니다.
    - 예시: Agent-Pro, Richelieu, Self-Evolving GPT
- 효과:
    메모리를 통한 경험 축적 및 반영은 에이전트가 새로운 과제에 대한 적응력과 일반화 능력을 크게 향상시키며, 테스트 타임에 보다 효과적으로 이전 경험을 활용하여 추론 품질을 높입니다.

4. Agentic System Search
- 기본 원리:
    에이전트 시스템 자체를 최적화하는 메타학습 접근법으로, 직접 솔루션 공간을 탐색하기보다는 에이전트의 내부 구조(프롬프트, 모듈, 전체 에이전트 시스템)를 개선하는 방식입니다.
- 검색 수준:
	1.	프롬프트 레벨:
    - 반복적 “검증 및 수정” 과정을 통해 프롬프트 자체를 최적화합니다.
    - 예: 진화적 프롬프트 최적화, 메타 프롬프트 반복 기법
	2.	모듈 레벨:
    - 에이전트 내에서 특정 기능(계획, 추론, 도구 활용, 메모리 관리)을 수행하는 모듈들을 재조합 및 최적화하여, 효율적인 협업 구조를 구성합니다.
    - 예: Agentsquare, Aflow
	3.	에이전트 레벨:
    - 전체 에이전트 시스템을 파이썬 코드 공간에서 탐색하며, 다중 에이전트 시스템 간 협업 및 최적화를 진행합니다.
    - 예: ADAS, GPTSwarm
- 효과:
    이러한 검색 기법들은 에이전트가 자체적으로 최적의 추론 전략을 구축할 수 있도록 도우며, 이를 통해 모델의 테스트 타임 추론 능력을 극대화합니다.
