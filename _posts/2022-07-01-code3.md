---
layout: post   
title: Pytorch to TFLite   
subtitle: Utils         
tags: [programming, pytorch, onnx, tflite, model convert]  
comments: true  
---   

Torch model을 tflite 모델로 변경하기 위해 우선 onnx 모델로 변경이 필요하다.
이때, 필요한 패키지들의 버전이 gpu cuda, cudnn과 버전 호환이 잘 되어야 변환된 tflite 모델도 gpu delegate를 오류 없이 쓸 수 있다.

## Requirements

* 테스트 환경
    * CUDA: v11.6
    * CUDNN: v8
    * GCC: v7.5.0
    * Python: v3.6
    

* Dependencies
    * torch==1.8.0+cu111
    * tensorflow==2.6.0
    * onnx==1.9.0
    * onnx-tf==1.9.0
    * onnxruntime


## 1. Torch to ONNX

```python

import torch
import onnx
import onnxruntime as ort
import numpy as np


model = TorchModel()
model.load_state_dict(torch.load(torch_weight))

model = model.cuda()
sample_input = torch.rand((batch_size, channel_size, height, width)).cuda()

# convert torch to onnx

torch.onnx.export(
    model,
    sample_input,
    "model.onnx",
    opset_version=12,
    input_names=['input'],
    output_names=['output']
)

# onnx inference test

ort_session = ort.InferenceSession("model.onnx")
output = ort_session.run(None,
                         {'input': np.random.randn(batch_size, channel_size, height, width).astype(np.float32)})

print("output shape: ", output.shape)

```

## 2. ONNX to Tensorflow

```python

import onnx
from onnx_tf.backend import prepare
import tensorflow as tf

onnx_model = onnx.load("model.onnx")

# convert onnx to tensorflow
tf_rep = prepare(onnx_model)
tf_rep.export_graph("checkpoints/")

# tensorflow inference test
model = tf.saved_model.load("checkpoints/")
model.trainable=False
input_tensor = tf.random.uniform([batch_size, channel_size, height, width])
out = model(**{"input": input_tensor})
print("output shape: ", out['output'].shape)

```

## 3. Tensorflow to TFLite

```python

import tensorflow as tf
import numpy as np

# convert tensorflow to tflite

converter = tf.lite.TFLiteConverter.from_saved_model("checkpoints/")
tflite_model = converter.convert()

with open("model.tflite", 'wb') as f:
  f.write(tflite_model)
  
# tflite inference test

interpreter = tf.lite.Interpreter(model_path="model.tflite")
interpreter.allocate_tensors()

input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

input_shape = input_details[0]['shape']
input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)
interpreter.set_tensor(input_details[0]['index'], input_data)
interpreter.invoke()

output_data = interpreter.get_tensor(output_details[0]['index'])
print("output shape: ", output_data.shape)
```