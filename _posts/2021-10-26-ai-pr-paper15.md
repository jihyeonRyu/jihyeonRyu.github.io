---
layout: post  
title: (MakeItTalk) Speaker-Aware Talking-Head Animation Rendering     
subtitle: AI Paper Review      
tags: [ai, ml, computer vision, GAN, facial animation, video synthesis, video generation, face generation]      
comments: true  
---  

[Source Link](https://github.com/yzhou359/MakeItTalk)  
[Paper Link](https://arxiv.org/pdf/2004.12992.pdf)  

![](./../assets/resource/ai_paper/paper15/1.png)  

ì˜¤ì§ í•œ ì¥ì˜ ì–¼êµ´ ì´ë¯¸ì§€ì™€ ì˜¤ë””ì˜¤ inputìœ¼ë¡œ ë§í•˜ëŠ” ì˜ìƒì„ ë§Œë“¤ì–´ë‚´ëŠ” ë°©ë²•ì„ ì œì•ˆí•œë‹¤.
ì˜¤ë””ì˜¤ë¡œë¶€í„° ì§ì ‘ì ìœ¼ë¡œ í”½ì…€ì„ ìƒì„±í•´ë‚´ëŠ” ê¸°ì¡´ì˜ ë°©ì‹ê³¼ ë‹¤ë¥´ê²Œ, ë¨¼ì € input ì˜¤ë””ì˜¤ì—ì„œ contentì™€ í™”ìì˜ ì •ë³´ë¥¼ ë¶„ë¦¬í•´ë‚¸ë‹¤.
ì˜¤ë””ì˜¤ contentëŠ” ê°•ì¸í•˜ê²Œ ì–¼êµ´ ì˜ì—­ê³¼ ì…ìˆ ì˜ ì›€ì§ì„ì„ ì»¨íŠ¸ë¡¤í•˜ê³ , í™”ìì— ëŒ€í•œ ì •ë³´ëŠ” ì–¼êµ´ì˜ í‘œì •ì˜ íŠ¹ì§•ê³¼ ê³ ê°œì˜ ì›€ì§ì„ì„ ì»¨íŠ¸ë¡¤í•œë‹¤. 
ì´ ë°©ë²•ì˜ ë˜ ë‹¤ë¥¸ ì£¼ìš” ìš”ì†ŒëŠ” ì–¼êµ´ì˜ ëœë“œë§ˆí¬ ì˜ˆì¸¡ì„ í†µí•´ í™”ìë¥¼ ë‹¤ì´ë‚˜ë¯¹í•˜ê²Œ ë°˜ì˜í•œë‹¤ëŠ” ê²ƒì´ë‹¤.
ì´ ì¤‘ê°„ì˜ í‘œí˜„ì„ í†µí•´ì„œ, ìš°ë¦¬ ë°©ë²•ì€ ë§ì€ ë§Œí™”, ê·¸ë¦¼ë“±ì„ í¬í•¨í•œ ë‹¤ì–‘í•œ portrait imageë¥¼ í•˜ë‚˜ì˜ unified frameworkë¥¼ í†µí•´ ì‘ë™í•œë‹¤.
ê²Œë‹¤ê°€ ì´ ë°©ë²•ì€ í•™ìŠµ ê³¼ì •ì—ì„œ ê´€ì°°í•  ìˆ˜ ì—†ì—ˆë˜ ì–¼êµ´ì´ë‚˜ ìºë¦­í„°ì— ëŒ€í•´ì„œë„ ì˜ ë™ì‘í•œë‹¤.

## Related Work 
### Audio-driven facial landmark synthesis
EskimezëŠ” ë…¸ì´ì¦ˆì— ê°•ì¸í•œ synchronized facial landmarkë¥¼ ìƒì„±í•´ë‚´ëŠ” ë°©ë²•ì„ ì œì•ˆí•œë‹¤. ê·¸ í›„ Chenì€ decoupled blockì„ í•™ìŠµì‹œì¼œ ë¨¼ì € landmarkë¥¼ ì–»ê³ ë‚˜ì„œ, rasterized videoë¥¼ ìƒì„±í•œë‹¤.
Attention maskë¥¼ í†µí•´ ê°€ì¥ ë§ì´ ë°”ë€ŒëŠ” ë¶€ë¶„ì— ì§‘ì¤‘(ì…ìˆ ) í•˜ê²Œë” í•œë‹¤. GreenwoodëŠ” forked Bi-directional LSTM networkë¡œ ì¶”ì¶œí•œ landmarkë¥¼ í†µí•´ì„œ jointlyí•˜ê²Œ ì–¼êµ´ í‘œì •ê³¼ ê³ ê°œ ëŒë¦°ë“¤ì„ í•™ìŠµí•˜ê²Œ í•œë‹¤. 
ëŒ€ë¶€ë¶„ì˜ ì´ì „ì˜ audio-to-face animation ì€ speech contentê°€ ì¼ì¹˜í•˜ëŠ”ë° ì§‘ì¤‘í•˜ê³ , í•™ìŠµ ì¤‘ ëª¨ë“œì˜ ë¶•ê°œë‚˜ í‰ê· í™”ë¥¼ í†µí•´ì„œ ìŠ¤íƒ€ì¼ì´ë‚˜ ì‹ ì› ì •ë³´ê°€ ëˆ„ë½ë˜ëŠ” ë¬¸ì œê°€ ë°œìƒí–ˆë‹¤.
ë°˜ë©´, ì´ ë…¼ë¬¸ì—ì„œ ì œì•ˆí•˜ëŠ” ë°©ë²•ì€ ì˜¤ë””ì˜¤ contentì™€ í™”ìì˜ ì •ë³´ë¥¼ ë¶„ë¦¬í•˜ì—¬, í™”ì-dependent í•œ ëœë“œë§ˆí¬ë¥¼ ì¶”ì¶œí•˜ì—¬ ì‚¬ìš©í•œë‹¤.

### Lip-sync facial animation
GPU íŒŒì›Œê°€ ì¦ê°€í•¨ì— ë”°ë¼ì„œ, audio-to-video ì˜ end-to-end í”„ë ˆì„ì›Œí¬ê°€ ê°€ëŠ¥í•˜ê²Œ ë˜ì—ˆë‹¤. 
Chenì€ cropped ëœ lipì˜ ì›€ì§ì…ì„ ê° í”„ë ˆì„ì—ì„œ í•©ì„±í•´ë‚´ëŠ” ë°©ë²•ì„ ì œì•ˆí–ˆë‹¤.
ê·¸ ë‹¤ìŒ Chungì€ GANì´ë‚˜ encoder-decoderë¥¼ í†µí•´ì„œ ì–¼êµ´ ì „ì²´ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•œë‹¤
ê·¸ëŸ¬ë‚˜ ì¡°ì‘ëœ ì–¼êµ´ ëª¨ë¸ì„ ìœ„í•œ ì˜¤ë””ì˜¤ ê¸°ë°˜ì˜ ë§í•˜ëŠ” ì´ˆìƒí™”ì˜ ê²½ìš° ì…ë ¥ ë§Œí™” ëª¨ë¸ì—ëŠ” ìˆ˜ë™ ì¡°ì‘ ë° ëŒ€ìƒ ë³€ê²½ë¿ ì•„ë‹ˆë¼ ë‚˜ë¨¸ì§€ ë¨¸ë¦¬ ë¶€ë¶„ì„ ì…ìˆ  ë„ˆë¨¸ë¡œ ì• ë‹ˆë©”ì´ì…˜í™”í•˜ê¸° ìœ„í•œ ì•„í‹°ìŠ¤íŠ¸ ê°œì…ì´ í•„ìš”í–ˆìŠµë‹ˆë‹¤. 
í•˜ì§€ë§Œ ì´ ë…¼ë¬¸ì˜ ë°©ë²•ì€ ë§¤ë‰´ì–¼í•œ inputì´ í•„ìš” ì—†ë‹¤. ë˜í•œ ìœ„ì˜ ë°©ë²•ì€ í™”ìì˜ ì •ì²´ì„±ì´ë‚˜ ìŠ¤íƒ€ì¼ì„ í¬ì°©í•˜ì§€ ëª»í•œë‹¤. ê²Œë‹¤ê°€ ë™ì¼í•œ ë¬¸ì¥ì„ ë‘ ê°œì˜ ë‹¤ë¥¸ ëª©ì†Œë¦¬ë¡œ ë§í•˜ë©´ ë” í‘œí˜„ë ¥ ìˆê³  ì‚¬ì‹¤ì ìœ¼ë¡œ ë§Œë“œëŠ” ë° í•„ìš”í•œ ì—­í•™ì´ ë¶€ì¡±í•˜ì—¬ ë™ì¼í•œ ì–¼êµ´ ì• ë‹ˆë©”ì´ì…˜ì„ ìƒì„±í•˜ëŠ” ê²½í–¥ì´ ìˆìŠµë‹ˆë‹¤.

### Style aware facial head animation
Swajanakornì´ ì œì•ˆí•œ ë°©ë²•ì€ re-timing ë‹¤ì´ë‚˜ë¯¹ í”„ë¡œê·¸ë˜ë°ì„ ì´ìš©í•˜ì—¬ í™”ìì˜ ì›€ì§ì„ì„ ì¬ìƒì‚°í•´ë‚´ëŠ”ë°, ì´ ë°©ë²•ì€ ì˜¤ì§ single subject ì— ëŒ€í•´ì„œë§Œ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤.
ë˜ë‹¤ë¥¸ ì´ë¥¸ ì—°êµ¬ì—ì„œ, LiuëŠ” ìƒ‰ê°, ê¹Šì´, ì˜¤ë””ì˜¤ë¥¼ ì‚¬ìš©í•˜ì—¬ facial animationì„ ìƒì„±í•œë‹¤. í•˜ì§€ë§Œ ì´ë°©ë²•ë„ ë³´ì§€ ëª»í–ˆë˜ í™”ìì— ëŒ€í•´ì„œ ì¼ë°˜í™” ì‹œí‚¬ ìˆ˜ ì—†ë‹¤.
CudeiroëŠ” í™”ìì˜ ìŠ¤íƒ€ì¼ì„ latent representationì— ìœ„ì¹˜ì‹œí‚¤ë ¤ ì‹œë„ í•œë‹¤. ThiesëŠ” ì‚¬ëŒì˜ ìŠ¤íƒ€ì¼ì„ static blendshape baseì— ì¸ì½”ë”©í•œë‹¤.
í•˜ì§€ë§Œ ë‘ ë°©ë²•ëª¨ë‘, lower facial animation (ì…ìˆ )ì—ë§Œ ì§‘ì¤‘í•˜ê³ , ì–¼êµ´ì˜ ì›€ì§ì„ì€ ì˜ˆì¸¡í•˜ì§€ ëª»í•œë‹¤.
ë” ì´ ë…¸ëˆ”ã„´ê³¼ ìœ ì‚¬í•œ ë°©ë²•ìœ¼ë¡œ ZhouëŠ” image ë„ë©”ì¸ì—ì„œ ë¶„ë¦¬í•œ identityì™€ contentë¥¼ audio-visual representationìœ¼ë¡œ jointly í•˜ê²Œ í•™ìŠµí•œë‹¤
í•˜ì§€ë§Œ, ê·¸ë“¤ì˜ identity ì •ë³´ëŠ” static facial appearanceì—ë§Œ ì§‘ì¤‘í•˜ê¸° ë•Œë¬¸ì— í™”ìì˜ ë‹¤ì´ë‚˜ë¯¹ì„ ë‹´ì§€ ëª»í•œë‹¤.
í•˜ì§€ë§Œ í™”ìì˜ ì¸ì‹ì€ ë‹¨ìˆœí•œ ê³ ì •ëœ ëª¨ìŠµì„ ë„˜ì–´ ë§ì€ ì¸¡ë©´ì„ í¬í•¨í•œë‹¤. ê°œì¸ì˜ facial expressionê³¼ head movementsëŠ” ëª¨ë‘ ì¤‘ìš”í•œ ìš”ì†Œë¡œ ì‘ìš©í•œë‹¤.
ì €ìì˜ ë°©ë²•ì€ í™”ìì˜ idë¥¼ static appearanceì™€ head motion dynamicsì™€ jointly í•˜ê²Œ í•™ìŠµì‹œí‚¨ë‹¤.

### Warpping-based character animation
FiserëŠ” ë¹„ë””ì˜¤ì™€ ê±°ê¸°ì„œ ë½‘ì€ landmarkë¥¼ í†µí•´ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•œë‹¤.
WengëŠ” ì‚¬ëŒì˜ templateë¥¼ fitting í•¨ìœ¼ë¡œì¨, ëª¨ì…˜ ìº¡ì³ ë°ì´íƒ€ë¥¼ ì´ìš©í•´ ì‚¬ëŒì˜ ì›€ì§ì„ì„ ë§Œë“¤ì–´ë‚¸ë‹¤.
ë°˜ë©´, ì´ ëª¨ë¸ì€ ì˜¤ì§ ì˜¤ë””ì˜¤ë¥¼ í†µí•´ì„œ í‘œì •ê³¼ ê³ ê°œì˜ ì›€ì§ì„ì„ í•©ì„±í•´ë‚¸ë‹¤. 

### Evaluation metrics
ê²€ì¦ì€ identity/styleì„ í‰ê°€í•˜ëŠ” ê²ƒì´ ë§¤ìš° ì¤‘ìš”í•˜ê³ , ë™ì‹œì— ì ì ˆí•œ ë²¤ì¹˜ë§ˆí¬ë¥¼ ì„¤ì •í•˜ëŠ” ê²ƒì€ ì¤‘ìš”í•˜ì§€ ì•Šë‹¤.
Agarwalì€ action unitì„ í†µí•´ì„œ styleì˜ ë¶„í¬ë¥¼ ì‹œê°í™”í•œë‹¤. 
í˜„ì¬ ì¡´ì¬í•˜ëŠ” quantitative metricsëŠ” pixel-levelì˜ artifactë¥¼ ì¡ì•„ë‚´ëŠ”ë° ë” ì§‘ì¤‘í–ˆë‹¤. 
Action unitì€ Gan-based ì ‘ê·¼ë°©ë²•ì—ì„œ í‘œí˜„ë ¹ì„ í‰ê°€í•˜ëŠ” ëŒ€ì•ˆìœ¼ë¡œ ì œì•ˆë˜ì—ˆë‹¤.
ì €ìëŠ” í‘œì •ê³¼ ê³ ê°œ ì›€ì§ì„ì„ í‰ê°€í•˜ê¸° ìœ„í•œ ê³ ì°¨ì›ì˜ ë‹¤ì´ë‚˜ë¯¹í•œ metricsì˜ ëª¨ìŒì„ ì œì•ˆí•œë‹¤.

### Image-to-image translation
image-to-image translationì€ talking face synthesis, editingì—ì„œ ê³µí†µì ìœ¼ë¡œ ë§ì´ ì‚¬ìš©í•˜ëŠ” ì ‘ê·¼ ë°©ë²•ì´ë‹¤.
Face2Faceì™€ VDubëŠ” ë‘ ê°œì˜ talking-head videos ì‚¬ì´ ê°•ì¸í•œ appearance transferì—fmf tngodgksek.
ê·¸ í›„ë¡œ adversarial trainingì´ ì±„íƒë˜ì–´ transferred ê²°ê³¼ë¥¼ ë” í–¥ìƒì‹œì¼°ë‹¤.
ì˜ˆë¥¼ë“¤ì–´ Kimì€ cycle-consistency lossë¥¼ ì‚¬ìš©í•˜ì—¬ styleì„ transferí•˜ì—¬ one-to-one transferì—ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ëŠ” ë°©ë²•ì„ ê³ ì•ˆí–ˆë‹¤.
ZakharovëŠ” few-shot learning ì„ ê°œë°œí•˜ì—¬ ìì—°ìŠ¤ëŸ¬ìš´ ì‚¬ëŒì˜ ì›€ì§ì„ì„ ìƒì„±í•˜ê¸° ìœ„í•´ landmarkë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì‚¬ìš©í•˜ì˜€ë‹¤.
ì´ëŸ° ê¸°ì¡´ì˜ ë°©ë²•ë“¤ì— ê¸°ë°˜í•˜ì—¬, ì €ìëŠ” image-to-image translationì„ ì±„íƒí•˜ì˜€ë‹¤. 
Zakharovì™€ ë‹¤ë¥´ê²Œ ì´ ëª¨ë¸ì€ fine-tuning ì—†ì´ í•™ìŠµ ê³¼ì •ì—ì„œ ë³´ì§€ ì•Šì•˜ë˜ ì–¼êµ´ë„ ìƒì„±í•´ë‚¼ ìˆ˜ ìˆë‹¤.
ë˜í•œ, image deformation ëª¨ë“ˆì„ í†µí•´ì„œ ìºë¦­í„° ì´ë¯¸ì§€ë„ ì ìš©í•  ìˆ˜ ìˆë‹¤.

### Disentangled learning
voice conversion communityì—ì„œ ì˜¤ë””ì˜¤ì— ìˆëŠ” contentì™€ styleì„ ë¶„ë¦¬í•´ ë‚´ëŠ” ê²ƒì€ ë§¤ìš° ë„“ê²Œ ì—°êµ¬ë˜ì–´ ì˜¤ê³  ìˆë‹¤.
ì´ì „ì˜ ì˜¤ë˜ëœ ì—­ì‚¬ë¥¼ ë³¼ í•„ìš”ì—†ì´ ìµœê·¼ì˜ ë°©ë²•ì´ í˜„ì¬ ë…¼ë¬¸ì— ì ìš©í•˜ê¸° ì í•©í•˜ë‹¤.
Wanì€ Resemblyzerë¥¼ ë‹¤ì–‘í•œ ì–¸ì–´ì—ì„œ ê²€ì¦ ëª©ì ìœ¼ë¡œ í™”ì ID ì„ë² ë”©ìœ¼ë¡œ ê°œë°œí•˜ì˜€ë‹¤.
Qianì€ AutoVCë¥¼ ì œì•ˆí•˜ì˜€ë‹¤. ì´ëŠ” few-shot voice conversion methodë¡œ ì˜¤ë””ì˜¤ë¥¼ contentì™¸ identity ì •ë³´ë¡œ ë¶„ë¦¬í•´ë‚¸ë‹¤.
ì´ ë…¼ë¬¸ì€ ë² ì´ìŠ¤ë¼ì¸ìœ¼ë¡œ AutoVCë¥¼ ì‚¬ìš©í•˜ì—¬ voice contentë¥¼ ì¶”ì¶œí•˜ê³  Resemblyzerë¥¼ ì´ìš©í•˜ì—¬ í™”ìì˜ id ì„ë² ë”©ì„ ì¶”ì¶œí•˜ëŠ”ë° ì‚¬ìš©í•œë‹¤.
ì´ ë…¼ë¬¸ì€ ìŒì„±ì„ ì˜¤ë””ì˜¤ ê¸°ë°˜ ì• ë‹ˆë©”ì´ì…˜ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ì•„ì´ë””ì–´ë¥¼ ì†Œê°œí•˜ê³  í™”ì ì¸ì‹ talking-head generationì˜ ì¥ì ì„ ì‹œì—°í•©ë‹ˆë‹¤.

## Method
![](./../assets/resource/ai_paper/paper15/2.png)  

MakeItTalk êµ¬ì¡°ëŠ” ì˜¤ë””ì˜¤ì™€ single facial imageë¥¼ ì‚¬ìš©í•˜ì—¬ speaker-aware talking head animationì„ ìƒì„±í•œë‹¤.
í•™ìŠµê³¼ì •ì—ì„œ, ì €ìëŠ” off-the-shelf face 3D landmark detectorë¥¼ ì‚¬ìš©í•˜ì—¬ input videoì—ì„œ landmarkë¥¼ ì¶”ì¶œí•œë‹¤. 
baseline ëª¨ë¸ì€ ì˜¤ë””ì˜¤ì™€ ì¶”ì¶œëœ landmarkë¥¼ ì´ìš©í•´ ì§ì ‘ì ìœ¼ë¡œ í•™ìŠµëœë‹¤.
í•˜ì§€ë§Œ ë†’ì€ ë™ì ì„±ì„ í™•ë³´í•˜ê¸° ìœ„í•´, ëœë“œë§ˆí¬ëŠ” ì½˜í…ì¸  í‘œí˜„ê³¼ ì–½íˆê³  ì˜¤ë””ì˜¤ ì‹ í˜¸ì˜ ìŠ¤í”¼ì»¤ ì„ë² ë”©ê³¼ ì–½í˜€ì„œ ì˜ˆì¸¡ë˜ì–´ì•¼ í•¨ì„ ë°œê²¬í–ˆë‹¤. 

íŠ¹íˆ, ì €ìëŠ” voice conversion neural networkë¥¼ ì‚¬ìš©í–ì—¬ speech contentì™€ identity informationì„ ë¶„ë¦¬ì‹œí‚¨ë‹¤. 

* contentëŠ” í™”ìì™€ëŠ” ê´€ê³„ ì—†ê³ , ì„ìˆ ê³¼ ê·¸ ì£¼ë³€ì˜ ì¼ë°˜ì ì¸ ì›€ì§ì„ì— ëŒ€í•´ì„œë§Œ ìº¡ì³í•´ì•¼í•œë‹¤.
    * ì˜ˆë¥¼ë“¤ì–´ ëˆ„ê°€ "Ha!' ë¼ê³  ë§í•˜ëŠ” ìƒê´€ì—†ì´ ì…ìˆ ì€ ë²Œë ¤ì§€ê¸¸ ê¸°ëŒ€ëœë‹¤ (content) 
* identityëŠ” í™”ìì— ë”°í•˜ ê²°ì •ë˜ë©°, ì›€ì§ì„ê³¼ ë‚˜ë¨¸ì§€ ê³ ê°œ ëŒë¦¼ ì›€ì§ì„ ê²°ì •í•œë‹¤.
    * ë‚˜ë¨¸ì§€, ëˆˆ, ì½”, ì…, ê³ ê°œ ì›€ì§ì„ì€ ì–´ë–¤ í™”ìê°€ ê·¸ ë§ì„ í•˜ëŠëƒì— ë”°ë¼ ë‹¤ë¥´ë‹¤ (identity) 

ë˜ìŠ¤í„°í™”ëœ ì´ë¯¸ì§€ë“¤ì„ ìƒì„±í•˜ê¸° ìœ„í•´ì„œ, í™”ìëŠ” landmark-to-image í•©ì„±ì„ ìœ„í•œ ë‘ê°€ì§€ ì•Œê³ ë¦¬ì¦˜ì„ ì œì•ˆí•œë‹¤.

ì‹¤ì œ ê·¸ë¦¼ ë“±ì˜ ì‚¬ëŒì´ ì•„ë‹Œ ì´ë¯¸ì§€ë¥¼ ìœ„í•´ì„œëŠ” Delaunary triangulation ì„ ì´ìš©í•˜ì—¬ ê°„ë‹¨í•œ image warpingì„ ìˆ˜í–‰í•œë‹¤. 
ì‹¤ì œ ì‚¬ëŒì´ë¯¸ì§€ë¥¼ ìœ„í•´ì„œëŠ” image-to-image translation networkë¥¼ ì‚¬ìš©í•˜ì—¬ (pix2pix) ìƒì„±í•œë‹¤. 

ëª¨ë“  ì´ë¯¸ì§€ í”„ë ˆì„ë“¤ê³¼ input audioë¥¼ ê°™ì´ í•©ì„±í•˜ë©´ ìµœì¢…ì˜ talking-head animationì„ ìƒì„±í•´ë‚¼ ìˆ˜ ìˆë‹¤.

### 1. Speech content Animation
ì˜¤ë””ì˜¤ì—ì„œ í™”ìë‘ ìƒê´€ì—†ëŠ” content representationì„ ì¶”ì¶œí•˜ê¸° ìœ„í•´ Qianì´ ê³ ì•ˆí•œ AutoVC encoderë¥¼ ì‚¬ìš©í•œë‹¤.
AutoVC ë„¤íŠ¸ì›Œí¬ëŠ” LSTM ê¸°ë°˜ì˜ ì¸ì½”ë”ë¥¼ ì‚¬ìš©í•˜ì—¬ input audioë¥¼ compact representationìœ¼ë¡œ ì••ì¶•í•˜ì—¬ ì›ë˜ì˜ í™”ìì˜ id ì •ë³´ëŠ” ë²„ë¦¬ê³  content ì •ë³´ëŠ” ë³´ì¡´í•˜ê²Œë” í•œë‹¤.
AutoVC ë„¤íŠ¸ì›Œí¬ì—ì„œ ì¶”ì¶œí•œ content embedding A (TxD) ë¼ê³  ì¹­í•œë‹¤.
* T: total number of audio frames
* D: Content dimension

content animation componentì˜ ëª©ì ì€ content embedding Aë¥¼ facial landmark positionìœ¼ë¡œ mapping í•˜ëŠ” ê²ƒì´ë‹¤. 
ì €ìëŠ” ì‹¤í—˜ì—ì„œ recurrent networkê°€ feedward networkë³´ë‹¤ í›¨ì”¬ ì„±ëŠ¥ì´ ì¢‹ìŒì„ ë°í˜”ë‹¤. 
ì™œëƒë©´ contentì™€ landmark ì‚¬ì´ì˜ sequentialí•œ ì˜ì¡´ì„±ì„ ìº¡ì³í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤.
ê°„ë‹¨í•œ RNNê³¼ LSTM ì¢…ë¥˜ë¡œ ì‹¤í—˜í•œ ê²°ê³¼ LSTMì´ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ëƒ„ì„ ë°í˜”ë‹¤.
ê° í”„ë ˆì„ tì—ì„œ, LSTM ëª¨ë“ˆì€ audio content Aë¥¼ ìœˆë„ìš° ì‚¬ì´ì¦ˆ (t -> t+ğœ)ë¡œ ë°›ëŠ”ë‹¤.
*  ğœ = 18 frames (a window size of 0.3s in our experiments)

3D landmark detectorë¡œ ì¶”ì¶œí•œ ì–´ë– í•œ input 3D static landmark q (68x3)ì™€ LSTMì˜ ê²°ê³¼ë¥¼ í•¨ê»˜ MLPë¥¼ í†µê³¼ì‹œì¼œ ìµœì¢… ì˜ˆì¸¡ì¸ Î”q ë¥¼ ì–»ëŠ”ë‹¤. 
ì´ëŠ” ê° í”„ë ˆì„ì˜ ë Œë“œë§ˆí¬ ëª¨ì…˜ì„ í‘œí˜„í•œë‹¤.

Speech content animation ëª¨ë“ˆ ëª¨ë¸ì€ sequentialí•œ ì˜ì¡´ì„±ì„ ê°€ì§€ê³  ì•„ë˜ì˜ transformationì„ í†µí•´ landmarkë¥¼ ìƒì„±í•œë‹¤.  
![](./../assets/resource/ai_paper/paper15/3.png)  

ì—¬ê¸°ì„œ LSTMì€ ì„¸ê°œì˜ layer unitì„ê°€ì§€ê³  ìˆê³ , ê°ê°ì˜ hidden state vectorì˜ ì‚¬ì´ì¦ˆëŠ” 256ì´ë‹¤.
decoder MLP ë„¤íŠ¸ì›Œí¬ëŠ” ì„¸ê°œì˜ layer ë¡œë˜ì–´ ìˆê³  ê°ê°ì˜ hidden state vector ì‚¬ì´ì¦ˆëŠ” 512, 256, 204(68x3)ì„ ê°€ì§„ë‹¤. 

### 2. Speaker-Aware Animation
ë‹¨ìˆœíˆ lip motion ë§Œ ë§¤ì¹­ í•˜ëŠ” ê²ƒì€ ì¶©ë¶„í•˜ì§€ ì•Šë‹¤.
ì–¼êµ´ì˜ ì›€ì§ì„ê³¼ ì…ê³¼ ê´€ë ¨ìˆëŠ” ëˆˆì¹ ë“±ì˜ ë‹¤ë¥¸ ì›€ì§ì„ë„ talking head ìƒì„±ì— ë” í’ë¶€í•œ íŒíŠ¸ê°€ ë  ìˆ˜ ìˆë‹¤. 

![](./../assets/resource/ai_paper/paper15/4.png)  

ìœ„ ê·¸ë¦¼ì€ ì„œë¡œ ë‹¤ë¥¸ í™”ìì˜ embeddingì„ ë³´ì—¬ì¤€ë‹¤. í•˜ë‚˜ëŠ” ì–¼êµ´ì„ ê±°ì˜ ì›€ì§ì´ì§€ ì•ŠëŠ” í™”ìì˜ ê²ƒì´ê³ , ë‹¤ë¥¸ í•˜ë‚˜ëŠ” ë” í™œë™ì ì¸ í™”ìì˜ ê²ƒì´ë‹¤.
ì €ìì˜ ë°©ë²•ì€ ì´ëŸ° ì„œë¡œ ë‹¤ë¥¸ í™”ìì˜ ì›€ì§ì„ì„ ì„±ê³µì ìœ¼ë¡œ ë¶„ë¦¬í•œë‹¤.

ì´ë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´, ë¨¼ì €, í™”ìì˜ identity ì„ë² ë”©ì„ speaker verification modelê³¼ í•¨ê»˜ ì¶”ì¶œí•œë‹¤.
ì´ëŠ” ì„œë¡œ ë‹¤ë¥¸ ë°œì–¸ì„ í•˜ëŠ” ê°™ì€ í™”ì ê°„ì˜ ìœ ì‚¬ì„±ì„ ìµœëŒ€í™”í•˜ê³  ì„œë¡œ ë‹¤ë¥¸ í™”ì ê°„ì˜ ìœ ì‚¬ì„±ì€ ìµœì†Œí™” í•¨ìœ¼ë¡œì¨ ë‹¬ì„±í•œë‹¤.

ì›ë˜ì˜ identity embedding vector ì‚¬ì´ì¦ˆëŠ” 256 ì´ë‹¤. 
ì €ìëŠ” ì´ dimensionalityë¥¼ í•˜ë‚˜ì˜ MLPë¥¼ í†µí•´ì„œ 256ì—ì„œ 128ë¡œ ì¶•ì†Œí•˜ì—¬ í•™ìŠµê³¼ì •ì—ì„œ ë³¼ ìˆ˜ ì—†ì—ˆë˜ í™”ìì— ëŒ€í•´ì„œë„ ì¼ë°˜í™” í•  ìˆ˜ ìˆëŠ” ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ì˜€ë‹¤.

ì£¼ì–´ì§„ identity embedding sê°€ ì¶”ì¶œë˜ë©´, í™”ìì˜ ì •ì²´ì„±ì„ ë°˜ì˜í•˜ë„ë¡ per-frame landmark pë¥¼ ì¶”ê°€ë¡œ ì¡°ì •í•œë‹¤. 
ë” ìì„¸íˆ ë§í•˜ë©´, ëœë“œë§ˆí¬ëŠ” í›ˆë ¨ ì¤‘ í™”ìì—ì„œ ê´€ì°°ë˜ëŠ” ë¨¸ë¦¬ ì›€ì§ì„ ë¶„í¬ ë° í‘œì • ì—­í•™ê³¼ ì¼ì¹˜í•˜ë„ë¡ êµë€ëœë‹¤.

ì´ëŸ° ë°©ë²•ìœ¼ë¡œ, ì¡°ì •ëœ ëœë“œë§ˆí¬ë¥¼ í†µí•´ì„œ ê·¸ëŸ´ë“¯í•œ ë¨¸ë¦¬ ì›€ì§ì„ì„ ë°˜ì˜í•œ speaker-specific ë¶„í¬ê°€ ìƒì„±ëœë‹¤.

speaker-Aware Animationë„ Speech content Animationëª¨ë“ˆê³¼ ë™ì¼í•œ LSTM êµ¬ì¡°ë¥¼ ì‚¬ìš©í•˜ì—¬ content vectorë¥¼ ìˆ˜ì •í•œë‹¤.
í•˜ì§€ë§Œ ì„œë¡œ ë‹¤ë¥¸ parameterë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” íš¨ìœ¨ì ì´ë¼ëŠ” ê²ƒì„ ë°í˜”ë‹¤. 
ê²°ê³¼ í‘œí˜„ì¸ cëŠ” ë¨¸ë¦¬ ì›€ì§ì„ê³¼ í‘œì • ì—­í•™ì„ ìº¡ì³í•˜ëŠ” ë° ë” ì í•©í•˜ë‹¤.

![](./../assets/resource/ai_paper/paper15/5.png)  

ì´ ëª¨ë“ˆì€ speaker embedding sì™€ ìˆ˜ì •ëœ content representation c, initial static landmark që¥¼ ì‚¬ìš©í•˜ì—¬ speaker aware landmarkë¥¼ ìƒì„±í•œë‹¤.

ë˜í•œ ì¼ì¹˜í•˜ëŠ” ì–¼êµ´ ì›€ì§ì„ê³¼ í‘œì •ì„ ìƒì„±í•˜ê¸° ìœ„í•´ì„œëŠ” speech content animation ëª¨ë“ˆì— ë¹„í•´ ê¸´ time-dependencies ê°€ í•„ìš”í•˜ë‹¤ëŠ” ê²ƒì„ ë°í˜”ë‹¤.
ìŒì†ŒëŠ” ì¼ë°˜ì ìœ¼ë¡œ ìˆ˜ì‹­ ë°€ë¦¬ì´ˆ ë™ì•ˆ ì§€ì†ë˜ì§€ë§Œ, ë¨¸ë¦¬ë™ì‘, ì˜ˆë¥¼ ë“¤ì–´ ë¨¸ë¦¬ê°€ ì™¼ìª½ì—ì„œ ì˜¤ë¥¸ìª½ìœ¼ë¡œ í”ë“¤ë¦¬ëŠ” ê²ƒì€ 1ì´ˆ ë˜ëŠ” ëª‡ì´ˆë™ì•ˆ ì§€ì†ë  ìˆ˜ ìˆë‹¤. 
ì´ëŸ° ê¸¸ê³  êµ¬ì¡°í™”ëœ ì˜ì¡´ì„±ì„ ìº¡ì³í•˜ê¸° ìœ„í•´, self-attention networkë¥¼ ë„ì…í•˜ì˜€ë‹¤.
self-attention layerëŠ” outputì„ weighted combination of learned per-frame representationì„ ê³„ì‚°í•œë‹¤.

ê°œì„ ëœ content representation cëŠ” speaker embedding sì™€ concatí•œë‹¤. ê° í”„ë ˆì„ì— í• ë‹¹ëœ ê°€ì¤‘ì¹˜ëŠ” window ë‚´ì˜ ëª¨ë“  ìŒ í”„ë ˆì„ í‘œí˜„ì„ ë¹„êµí•˜ëŠ” í˜¸í™˜ì„± í•¨ìˆ˜ì— ì˜í•´ ê³„ì‚°ëœë‹¤.
*  window size ğœâ€² = 256 frames (4 sec)

self-attention layerì˜ ë§ˆì§€ë§‰ outputê³¼ initial static landmarkëŠ” MLPë¥¼ í†µí•´ì„œ ë§ˆì§€ë§‰ per-frame landmarkë¥¼ ê³„ì‚°í•œë‹¤.  
![](./../assets/resource/ai_paper/paper15/6.png)  
ì—¬ê¸°ì„œ attention networkëŠ” Vaswaniì˜ ê²ƒì„ ì ìš©í•˜ì˜€ë‹¤.

### 3. Single-Image Animation
ì£¼ì–´ì§„ input image Qì™€ ì˜ˆì¸¡ëœ ëœë“œë§ˆí¬ set ì¸ yë¥¼ ì´ìš©í•˜ì—¬ sequence of image Fë¥¼ ìƒì„±í•œë‹¤.
ì…ë ¥ portraitì€ ë§Œí™” ì–¼êµ´ ë˜ëŠ” ìì—°ìŠ¤ëŸ¬ìš´ ì‚¬ëŒ ì´ë¯¸ì§€ë¥¼ ë¬˜ì‚¬í•  ìˆ˜ ìˆë‹¤.
í™”ìëŠ” ê° ë‘ íƒ€ì…ì— ëŒ€í•´ì„œ ì„œë¡œë‹¤ë¥¸ ë°©ë²•ì„ ì ìš©í•œë‹¤.

#### Cartoon Images (non-photorealistic)
ì´ëŸ° ì´ë¯¸ì§€ëŠ” ë³´í†µ sharpí•œ íŠ¹ì§•ì˜ edgeë¥¼ ê°€ì§„ë‹¤.
ì´ëŸ° sharp í•œ íŠ¹ì§•ì„ ë³´ì¡´í•˜ê¸° ìœ„í•´, ì €ìëŠ” morphing ê¸°ë°˜ì˜ ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ pixel-levelì˜ artifactë¥¼ í”¼í•œë‹¤.

ì…ë ¥ ì´ë¯¸ì§€ë¡œë¶€í„°, í™”ìëŠ” facial landmarkë¥¼ ì¶”ì¶œí•˜ê³ , Delaunay triangulationì„ ì‚¬ìš©í•˜ì—¬ semantic triangle ì„ ìƒì„±í•œë‹¤. 
texture mapìœ¼ë¡œì¨ì˜ ì´ˆê¸°ì˜ í”½ì…€ë“¤ì€ triangleë¡œ ë§µí•‘ëœë‹¤. ê·¸ë¦¬ê³  í›„ì† ì• ë‹ˆë§¤ì´ì…˜ í”„ë¡œì„¸ìŠ¤ëŠ” ì‹¬í”Œí•˜ë‹¤.

ëœë“œë§ˆí¬ í† í´ë¡œì§€ê°€ ë™ì¼í•˜ê²Œ ìœ ì§€ë˜ëŠ” í•œ ê° ì‚¼ê°í˜•ì˜ í…ìŠ¤ì²˜ëŠ” í”„ë ˆì„ì„ í†µí•´ ìì—°ìŠ¤ëŸ½ê²Œ ì „ì†¡ëœë‹¤.
ì €ìëŠ” ì´ë¥¼ GLSL-based C++ ì½”ë“œì— ì ìš©í•˜ì—¬ vertex/fragment shadersë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ìˆ˜í–‰í•  ìˆ˜ ìˆì—ˆë‹¤

![](./../assets/resource/ai_paper/paper15/7.png)  

#### Natural Images
Zakharvê°€ ì œì•ˆí•œ landmark ê¸°ë°˜ì˜ facial animation í•©ì„± ë°©ë²•ì—ì„œ ì°©ì•ˆí•˜ì˜€ë‹¤. 

target face appearanceë¥¼ ì¸ì½”ë”© í•˜ê¸° ìœ„í•´ embedderê³¼ adaptive instance normalization layerë¥¼ ë¶„ë¦¬í•˜ì—¬ ì‚¬ìš©í•˜ëŠ” ëŒ€ì‹ , 
UNet êµ¬ì¡°ì— ê¸°ë°˜í•˜ì—¬ ë³€ìœ„ëœ ëœë“œë§ˆí¬ì™€ portraitì„ ì²˜ë¦¬í•œë‹¤.
íŠ¹íˆ, ë¨¼ì € ì—°ì†ì ì¸ ì–¼êµ´ ëœë“œë§ˆí¬ë¥¼ ì—°ê²°í•˜ê³  ë¯¸ë¦¬ ì •ì˜ëœ ìƒ‰ìƒì˜ ì„ ë¶„ìœ¼ë¡œ ëœë”ë§í•˜ì—¬ ì˜ˆì¸¡ëœ ëœë“œë§ˆí¬ yì˜ ì´ë¯¸ì§€ í‘œí˜„ Yë¥¼ ìƒì„±í•œë‹¤. 
image YëŠ” input portrait image Qì™€ channel-wise í•˜ê²Œ concatenatedí•˜ì—¬ 256x256ì˜ 6-channel ì´ë¯¸ì§€ë¥¼ ìƒì„±í•œë‹¤.
ê·¸ë¦¬ê³  encoder-decoder ë„¤íŠ¸ì›Œí¬ë¥¼ í†µí•´ 256x256ì˜ ì´ë¯¸ì§€ë¥¼ ê°ê° ìƒì„±í•´ë‚¸ë‹¤
generatorì˜ êµ¬ì¡°ëŠ” Esserì™€ Hanì˜ ë°©ì‹ì„ ë”°ë¥´ê³  ìˆë‹¤.
íŠ¹íˆ, encoderëŠ” 6ê°œì˜ convolution layerë¥¼ ê°€ì§€ê³ , ê°ê° 2-stride, two-residual block êµ¬ì¡°ë¥¼ ê°€ì§€ê³  ìˆë‹¤. 
decoderëŠ” symmetric upsampling blockì„ ì‚¬ìš©í•œë‹¤.
skip-layerëŠ” encoderì™€ decoderë¥¼ ì—°ê²°í•˜ì—¬ ìµœì í™”ë¥¼ ìˆ˜í–‰í•œë‹¤.
ëœë“œë§ˆí¬ëŠ” ì‹œê°„ì´ ì§€ë‚¨ì— ë”°ë¼ ë¶€ë“œëŸ½ê²Œ ë³€í•˜ê¸° ë•Œë¬¸ì— ìµœì¢… ê²°ê³¼ ì´ë¯¸ì§€ëŠ” temporal coherenceë¥¼ ë³´ì—¬ì¤€ë‹¤.
Â®
## Training
### 1. Voice Conversion Training
![](./../assets/resource/ai_paper/paper15/8.png)  

í•™ìŠµ ì…‹íŒ…ì€ [Qian](https://github.com/auspicious3000/autovc) ì´ ì œì•ˆí•œ ë°©ì‹ëŒ€ë¡œ í•˜ì˜€ê³ 
 speaker embeddingì€ Wanì´ ì œê³µí•œ pre-trained modelì„ í†µí•´ì„œ initializeë¥¼ ìˆ˜í–‰í•˜ì˜€ë‹¤.
í•™ìŠµ ì†ŒìŠ¤ speechëŠ” content encoderë¥¼ í†µí•´ ê³„ì‚°ëœë‹¤.
ê°™ì€ í™”ìë¡œë¶€í„° ë‹¤ë¥¸ ìŒì†ŒëŠ” speaker embedding ì„ ì¶”ì¶œí•˜ê¸° ìœ„í•´ ì‚¬ìš©ëœë‹¤.
audio content embeddingì„ decoderê°€ ë‹¤ì‹œ ì˜¤ë¦¬ì§€ë„ ì†ŒìŠ¤ë¡œ ì¬ê±´í•˜ê²Œë” í•™ìŠµí•œë‹¤.
content encoder, decoder, mlpëŠ” self-reconstruction errorë¥¼ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ í•™ìŠµëœë‹¤.
í•™ìŠµì€ VCTK corpusë¡œ ìˆ˜í–‰í•˜ì˜€ë‹¤. (ë‹¤ì–‘í•œ ìƒí™©ì—ì„œì˜ 109 ëª…ì˜ ë¯¸êµ­ì¸ì˜ ìŒì†Œ ë°ì´í„°ì…‹)

### 2. Speech Content Animation Training

#### Dataset
ë†’ì€ í’ˆì§ˆì˜ facial landmarkì™€ ì´ì— ë§ëŠ” audioë¥¼ ì œê³µí•˜ëŠ” audio-visual ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ í•™ìŠµí•œë‹¤.
ì´ë¥¼ ìœ„í•´ Obama Weekly Address ë°ì´í„°ì…‹ (6ì‹œê°„ì˜ ê¸´ ì˜¤ë°”ë§ˆ ì—°ì„¤)ì„ ì‚¬ìš©í•˜ì˜€ë‹¤.
ê³ í™”ì§ˆê³¼ ìƒëŒ€ì ìœ¼ë¡œ ì¹´ë©”ë¼ì˜ ì •ë©´ì„ ë°”ë¼ë³´ê¸° ë•Œë¬¸ì— ì •í™•í•œ facial landmarkë¥¼ ì°¾ì„ ìˆ˜ ìˆë‹¤.

ë˜í•œ ê°€ì¥ ì˜ ì¶”ì •ëœ affine transformationì„ ì‚¬ìš©í•˜ì—¬ ì •ë©´ í‘œì¤€ ì–¼êµ´ í…œí”Œë¦¿ì— ì–¼êµ´ ëœë“œë§ˆí¬ë¥¼ ë“±ë¡í•œë‹¤.
ì´ëŠ” í™”ì ì¢…ì† ë¨¸ë¦¬ í¬ì¦ˆ ëª¨ì…˜ì„ ê³ ë ¤í•˜ëŠ” ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ìˆë‹¤.

ì—¬ê¸°ì— ìš°ë¦¬ì˜ ëª©í‘œëŠ” ì˜¤ë””ì˜¤ë¥¼ ê¸°ë°˜í•˜ëŠ” ì¼ë°˜ì ì¸ ë¦½ëª¨ì…˜ì„ ë°°ìš°ëŠ” ê²ƒì´ê¸° ë•Œë¬¸ì— í™”ì í•œëª…ì´ ì´ ëª¨ë“ˆì„ í›ˆë ¨í•˜ëŠ” ë° ì¶©ë¶„í•˜ë‹¤ëŠ” ì ì„ ê°•ì¡°í•œë‹¤.

ë¦½ì‹±í¬ëŠ” í™”ì ì¸ì‹ ë¶„ê¸°ë¥¼ í¬í•¨í•˜ê³  ì—¬ëŸ¬ ì¸ê°„ ì£¼ì œì— ëŒ€í•œ êµìœ¡ì„ í¬í•¨í•˜ì—¬ íŠ¹ì • í™”ì IDì— íŠ¹í™”ë©ë‹ˆë‹¤.

#### Loss function
reference landmark positionê³¼ predicted landmark ê°„ì˜ ê±°ë¦¬ë¥¼ ìµœì†Œí™” (a) í•˜ê³ , ëœë“œë§ˆí¬ì˜ ì˜¬ë°”ë¥¸ ë°°ì¹˜ë¥¼ ì´‰ì§„í•˜ê³  ì–¼êµ´ ëª¨ì–‘ì˜ ì„¸ë¶€ ì‚¬í•­ì„ ë³´ì¡´í•˜ê¸° ìœ„í•´ graph Laplacian coordinateì˜ ê±°ë¦¬ë¥¼ ìµœì†Œí™”(b) í•œë‹¤.  
![](./../assets/resource/ai_paper/paper15/9.png)  
* ğœ†ğ‘ =1 in our implementation

![](./../assets/resource/ai_paper/paper15/10.png)  
* N(p): include the landmark neighbors conncected to pi within a distinct facial part 
![](./../assets/resource/ai_paper/paper15/11.png)  

ì €ìëŠ” 8ê°œì˜ facial partë¥¼ ì‚¬ìš©í•˜ì˜€ë‹¤.

### 3. Speaker-Aware Animation Training
#### Dataset
í™”ì ì¸ì‹ ë‹¤ì´ë‚˜ë¯¹í•œ ì–¼êµ´ì˜ ì›€ì§ì„ê³¼ í‘œì •ì„ í•™ìŠµí•˜ê¸° ìœ„í•´ ë‹¤ì–‘í•œ í™”ìì˜ íŠ¹ì§•ì´ ìˆëŠ” audio-visual datasetì„ ì‚¬ìš©í•˜ì˜€ë‹¤. 
VoxCeleb2 ë°ì´í„°ì…‹ ë‹¤ì–‘í•œ í™”ìì— ëŒ€í•œ segmentation ì •ë³´ë„ ì œê³µí•´ì„œ ìš°ë¦¬ì˜ ëª©ì ì— ì í•©í•˜ë‹¤ê³  íŒë‹¨í–ˆë‹¤.ì´
VoxCeleb2ëŠ” ì›í•´ speacker verificationì„ ìœ„í•´ ë””ìì¸ë˜ì—ˆë‹¤. 
ìš°ë¦¬ì˜ ëª©ì ì€ ë§í•˜ëŠ” ì–¼êµ´ì„ í•©ì„±í•˜ê¸° ìœ„í•´ ë™ì ì¸ ëª¨ìŠµì„ ìº¡ì³í•˜ëŠ” ê²ƒì´ê¸° ë•Œë¬¸ì—, ì´ 1,232 ë¹„ë””ì˜¤ í´ë¦½ì—ì„œ 67 ëª…ì˜ í™”ì subsetì„ ì¶”ì¶œí•˜ì—¬ ì‚¬ìš©í•˜ì˜€ë‹¤.
í‰ê· ì ìœ¼ë¡œ 5-10ë¶„ì •ë„ì˜ ë¹„ë””ì˜¤ í´ë¦½ì„ ì„ íƒí•˜ì˜€ë‹¤.
ì„ íƒ ê¸°ì¤€ì€, ëœë“œë§ˆí¬ detectionì´ ì •í™•í•œ ê²ƒì´ë‹¤.
í™”ìëŠ” speaker represntation spaceì—ì„œ Poisson disk sampling ì„ í†µí•´ ì„ ë³„í•˜ì˜€ë‹¤.
ë°ì´í„°ì…‹ì„ 60%/20%/20% ë¹„ìœ¨ë¡œ ë‚˜ëˆ„ì–´ì„œ ê°ê° í•™ìŠµ, hold-out-validation, testingì— ê°ê° ì‚¬ìš©í•˜ì˜€ë‹¤.
content animation stepê³¼ ë‹¤ë¥´ê²Œ, ì „ì²´ì ì¸ ì–¼êµ´ì˜ ì›€ì§ì„ì„ í•™ìŠµí•´ì•¼ í•˜ê¸° ë•Œë¬¸ì— ëœë“œë§ˆí¬ë¥¼ front face templateì— register í•˜ì§€ ì•Šì•˜ë‹¤. 

#### Adversarial network
ëœë“œë§ˆí¬ ìœ„ì¹˜ë¥¼ í¬ì°©í•˜ëŠ” ê²ƒ ì´ì™¸ì—ë„, í›ˆë ¨ì¤‘ í™”ìì˜ ë¨¸ë¦¬ ì›€ì§ì„ê³¼ í‘œì • ì—­í•™ì„ ì¼ì¹˜ì‹œí‚¤ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•œë‹¤.
ì´ë¥¼ ìœ„í•´ì„œ GAN ë°©ì‹ì„ ì‚¬ìš©í•˜ì˜€ë‹¤. íŠ¹íˆ, discriminator network Attn_dë¥¼ ì°¨ìš©í•˜ì˜€ëŠ”ë°, ì´ëŠ” self-attention generator networkì™€ ë¹„ìŠ·í•œ êµ¬ì¡°ë¥¼ ê°€ì§€ê³  ìˆë‹¤.
Discriminatorì˜ ëª©ì ì€ í™”ìì˜ facial landmarkì˜ temporal ì›€ì§ì„ì´ ì§„ì§œì²˜ëŸ¼ ë³´ì´ëŠ” ì§€ ì°¾ëŠ”ë° ìˆë‹¤.
ì…ë ¥ìœ¼ë¡œ facial landmarkë¥¼ generatorì—ì„œ ì‚¬ìš©í–ˆë˜ ê°™ì€ window ë¥¼ ì‚¬ìš©í•˜ì—¬ audio contentì™€ speacker's embedding ê³¼ í•¨ê»˜ sequencial í•˜ê²Œ ë°›ëŠ”ë‹¤. 
ì¶œë ¥ì€ í”„ë ˆì„ë‹¹ realism r_t ë¡œ íŠ¹ì •ëœë‹¤.

![](./../assets/resource/ai_paper/paper15/12.png)  

LSGAN lossë¥¼ ì‚¬ìš©í•˜ì—¬ discriminatorë¥¼ í•™ìŠµì‹œí‚¨ë‹¤. 
![](./../assets/resource/ai_paper/paper15/13.png)  

#### Loss function
self-attention generator networkë¥¼ í•™ìŠµí•˜ê¸° ìœ„í•´, output landmarkì˜ realismì„ ìµœëŒ€í™”í•˜ê¸°ë¥¼ ì‹œë„í•œë‹¤.
ë˜í•œ ì ˆëŒ€ ìœ„ì¹˜ì™€ ë¼í”Œë¼ì‹œì•ˆ ì¢Œí‘œì˜ ê´€ì ì—ì„œì˜ ê±°ë¦¬ë„ í›ˆë ¨ì—ì„œ ê³ ë ¤í•œë‹¤.

![](./../assets/resource/ai_paper/paper15/14.png)
* ğœ†ğ‘  = 1, ğœ‡ğ‘  = 0.001

ë‹¤ë¥¸ GAN ì ‘ê·¼ ë°©ì‹ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ discriminatorê³¼ generatorë¥¼ alternating í•˜ë©´ì„œ í•™ìŠµí•˜ì˜€ë‹¤. 

### 4. Image-to-Image Translation Training
VoxCeleb2ì—ì„œ ì¶”ì¶œí•œ ë¹„ë””ì˜¤ í”„ë ˆì„ì˜ ìŒì„ encoder/decoder pairë¡œ ì‚¬ìš©í•˜ì—¬ í•™ìŠµì‹œì¼°ë‹¤.
ë„¤íŠ¸ì›Œí¬ë¥¼ fine-tuneí•˜ê¸° ìœ„í•´ [Siarohin](https://papers.nips.cc/paper/2019/file/31c0b36aef265d9221af80872ceb62f9-Paper.pdf) ì´ ì œê³µí•œ ê³ í™”ì§ˆ ë¹„ë””ì˜¤ cropì„ ì‚¬ìš©í•˜ì˜€ë‹¤.

ëœë¤í•˜ê²Œ frame pair(QË†ğ‘ ğ‘Ÿğ‘ source, QË†ğ‘¡ğ‘Ÿğ‘” target)ì„ ìƒ˜í”Œë§í•˜ê³  targetì— ëŒ€í•œ ëœë“œë§ˆí¬ë¥¼ ì¶”ì¶œí•˜ê³  ì´ë¥¼ RGB ì´ë¯¸ì§€ë¡œ ë³€í™˜í•˜ì—¬ YË†ğ‘¡ğ‘Ÿğ‘” ë¥¼ ì–»ëŠ”ë‹¤.
QË†ğ‘ ğ‘Ÿğ‘ ì™€  YË†ğ‘¡ğ‘Ÿğ‘”ë¥¼ concate í•˜ì—¬ inputìœ¼ë¡œ ì‚¬ìš©í•˜ê³  ìµœì¢… ê²°ê³¼ì¸ Qğ‘¡ğ‘Ÿğ‘” ë¥¼ ì–»ëŠ”ë‹¤

![](./../assets/resource/ai_paper/paper15/15.png)
* ğœ†ğ‘ = 1, and ğœ™ concatenates feature map activations from the pretrained VGG19 network

![](./../assets/resource/ai_paper/paper15/16.png)  


### 5. Implementation Details
ëª¨ë“  ëœë“œë§ˆí¬ëŠ” 62.5 fpsë¡œ ì¡°ì •í•˜ê³  ì˜¤ë””ì˜¤ì˜ waveformì€ 16K Hz frequencyë¡œ ìƒ˜í”Œë§í•˜ì˜€ë‹¤.
ë‹¤ë¥¸ fps ë„ í…ŒìŠ¤íŠ¸ í•´ë´¤ì§€ë§Œ ìœ„ì˜ ë¹„ìœ¨ì´ ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì•˜ë‹¤, 
ëœë“œë§ˆí¬ëŠ” ì…ë ¥ ë¹„ë””ì˜¤ì˜ ì˜¤ë¦¬ì§€ë„ fpsë¡œ ì¶”ì¶œì„ í•˜ì˜€ê³ , interpolationì€ ì›ë˜ì˜ pixelì´ ì•„ë‹Œ ëœë“œë§ˆí¬ì—ì„œ ìˆ˜í–‰ë˜ì—ˆë‹¤. 
contentì™€ speaker ëª¨ë“ˆ ë‘˜ë‹¤ Adam ê³¼ pytorchë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµì‹œì¼°ìœ¼ë©°, lrëŠ” 10-4, weight decayëŠ” 10-6ë¡œ ì„¤ì •í•˜ì˜€ë‹¤. 
ì†ë„ëŠ” human face ê¸°ì „ 22 FPS ì´ë‹¤. 

## Evaluation Protocol
![](./../assets/resource/ai_paper/paper15/17.png)  

í•™ìŠµ ê³¼ì •ì—ì„œ ë™ì¼í•œ ì‹ ì›ì€ ê´€ì°° ë  ìˆ˜ ìˆì§€ë§Œ, ì—°ì„¤ê³¼ ë¹„ë””ì˜¤ëŠ” ë³¸ ì ì—†ëŠ” ì…‹ìœ¼ë¡œ êµ¬ì„±í•˜ì˜€ë‹¤.

**Evaluation Metrics** : í•©ì„±ëœ ëœë“œë§ˆí¬ê°€ ì •í™•í•œ ì…ìˆ  ì›€ì§ì„ì„ ì–¼ë§ˆë‚˜ ì˜ ë‚˜íƒ€ë‚´ëŠ” ì§€ í‰ê°€í•œë‹¤. reference landmarkëŠ” [Bulat and Tzimiropoulos](https://openaccess.thecvf.com/content_ICCV_2017/papers/Bulat_How_Far_Are_ICCV_2017_paper.pdf) ê°€ ì œì•ˆí•œ ë°©ë²•ì„ ì‚¬ìš©í•˜ì˜€ë‹¤.

* Landmark distance for jaw-lips (D-LL): í„±, ì… ë“±ì˜ ì˜ˆì¸¡ëœ ëœë“œë§ˆí¬ì™€ reference landmark ê°„ì˜ ìœ í´ë¦¬ë””ì•ˆ ê±°ë¦¬ í‰ê· , ëœë“œë§ˆí¬ ìœ„ì¹˜ëŠ” í´ë¦½ë³„ reference lipì˜ ìµœëŒ€ widthë¡œ normalize í•œë‹¤.
* Landmark Velocity difference for jaw-lips (D-VL): reference ì™€ ì˜ˆì¸¡í•œ ëœë“œë§ˆí¬ì˜ ì†ë„ì— ëŒ€í•œ ìœ í´ë¦¬ë””ì•ˆ ê±°ë¦¬ í‰ê· ìœ¼ë¡œ í‰ê°€í•œë‹¤. ì†ë„ëŠ” ì—°ì† í”„ë ˆì„ê°„ì˜ ëœë“œë§ˆí¬ ìœ„ì¹˜ ì°¨ì´ë¡œ ê³„ì‚°ëœë‹¤. ë©”íŠ¸ë¦­ì€ 1ì°¨ jaw-lip dynamicì„ í¬ì°©í•œë‹¤.
* Difference in open mouth area (D-A): ì˜ˆì¸¡ê³¼ referenceì˜ ì…ëª¨ì–‘ ì˜ì—­ê°„ì˜ ì°¨ì´ë¡œ ê³„ì‚°í•œë‹¤. ê° í…ŒìŠ¤íŠ¸ ë¹„ë””ì˜¤ í´ë¦½ì— ëŒ€í•œ ê¸°ì¤€ ì…ì˜ ìµœëŒ€ ë©´ì ì˜ ë°±ë¶„ìœ¨ë¡œ í‘œì‹œëœë‹¤.

ì–¼êµ´ì˜ ì›€ì§ì„ê³¼ í‘œì •, ì—­ë™ì„±ì„ ì–¼ë§ˆë‚˜ ì˜ ìƒì„±í•´ ë‚´ëŠ” ì§€ í‰ê°€í•œë‹¤.

* Landmark distance(L-D): ëª¨ë“  ì˜ˆì¸¡ëœ ëœë“œë§ˆí¬ì™€ reference ê°„ì˜ ìœ í´ë¦¬ë””ì•ˆ ê±°ë¦¬ì°¨ë¡œ ê³„ì‚°, ì–¼êµ´ì˜ ë„“ì´ë¡œ normalize ìˆ˜í–‰
* Landmark velocity difference (D-V): ì˜ˆì¸¡ê³¼ reference ëœë“œë§ˆí¬ì˜ velocity ê°„ì˜ í‰ê·  ìœ í´ë¦¬ë””ì•ˆ ê±°ë¦¬, ì–¼êµ´ ë„“ì´ë¡œ normalize ìˆ˜í–‰. 
* Head rotation and position difference (D-Rot/Pos): ì–¼êµ´ì˜ ëŒì•„ê°„ ê°ë„ì˜ ì°¨ì´(degree), ìœ„ì¹˜ ì°¨ì´ë¥¼ ê³„ì‚°.

































